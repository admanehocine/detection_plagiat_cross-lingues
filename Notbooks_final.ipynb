{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsMsA5p2dM9X"
      },
      "source": [
        "#1) PIP Commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2jz-hS-dKJ3",
        "outputId": "7ebe11df-975d-41fd-e073-82e8139391c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask_wtf\n",
            "  Downloading Flask_WTF-1.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting WTForms\n",
            "  Downloading WTForms-3.0.1-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: itsdangerous in /usr/local/lib/python3.7/dist-packages (from flask_wtf) (1.1.0)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from flask_wtf) (1.1.4)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->flask_wtf) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->flask_wtf) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask->flask_wtf) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask->flask_wtf) (2.0.1)\n",
            "Installing collected packages: WTForms, flask-wtf\n",
            "Successfully installed WTForms-3.0.1 flask-wtf-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-bidi\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from python-bidi) (1.15.0)\n",
            "Installing collected packages: python-bidi\n",
            "Successfully installed python-bidi-0.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting arabic-reshaper\n",
            "  Downloading arabic_reshaper-2.1.3-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from arabic-reshaper) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from arabic-reshaper) (57.4.0)\n",
            "Installing collected packages: arabic-reshaper\n",
            "Successfully installed arabic-reshaper-2.1.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0.7)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.48.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pyarabic) (1.15.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.15\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 24.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 63.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 66.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 46.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=487b72e65a46b0ec9defa29b5e10a0b95331d1e5aa2dc72ff98e5e2a5d472047\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.9.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.22.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting translate-toolkit\n",
            "  Downloading translate_toolkit-3.7.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.6.3 in /usr/local/lib/python3.7/dist-packages (from translate-toolkit) (4.9.1)\n",
            "Installing collected packages: translate-toolkit\n",
            "Successfully installed translate-toolkit-3.7.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.22.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.97)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.7.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.9.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.7)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "--2022-09-21 06:45:41--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 52.202.168.65, 18.205.222.128, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13770165 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.tgz’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  52.3MB/s    in 0.3s    \n",
            "\n",
            "2022-09-21 06:45:41 (52.3 MB/s) - ‘ngrok-stable-linux-amd64.tgz’ saved [13770165/13770165]\n",
            "\n",
            "ngrok\n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install flask_wtf\n",
        "!pip install python-bidi\n",
        "!pip install --upgrade arabic-reshaper\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install --user -U nltk\n",
        "!pip install --user -U numpy\n",
        "!pip install pyarabic\n",
        "!pip install sentence_transformers\n",
        "!pip install transformers\n",
        "!pip install translate-toolkit\n",
        "!pip install sentence_transformers\n",
        "\n",
        "# Ngrok Server client authentificatioçn\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz\n",
        "!tar -xvf /content/ngrok-stable-linux-amd64.tgz\n",
        "!./ngrok authtoken 2Er5sHdMaEwPpSxmspDdO0cR9Rr_276BLXZakSeFRZ46nnDhZ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5CoynMDdU6h"
      },
      "source": [
        "#2) Imports + Global Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0RjwQVPNNQw"
      },
      "source": [
        "##2.1) Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SbMJVoPdZxQ",
        "outputId": "5626d40f-86a0-4265-8bda-58a02ca841da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moving 0 files to the new cache system\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "593ee0396f244231a09e51f698c0db73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# usual Packages\n",
        "import numpy as np\n",
        "import re\n",
        "import sys\n",
        "import os\n",
        "import gc\n",
        "\n",
        "from sklearn.utils import shuffle # To randomize dataset lines\n",
        "from math import floor \n",
        "import pyarabic.araby as araby    # Using araby to remove diacritics in Arabic\n",
        "\n",
        "# Natural Language toolkit\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Ginsim module (Word2Vec)\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "# Keras module (LSTM and GRU models)\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, LSTM, GRU, Embedding, Dropout, Flatten, Input\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras import callbacks\n",
        "from keras.losses import CosineSimilarity\n",
        "\n",
        "# Sentence transformer module (BERT Model)\n",
        "from sentence_transformers import SentenceTransformer, models, InputExample, losses, util, evaluation,SentencesDataset # BERT Model\n",
        "from torch import nn, cuda # So we can use GPU\n",
        "import torch\n",
        "from torch.utils.data import DataLoader #Torch data formater, works with BERT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5LHhPCYm2O_"
      },
      "source": [
        "##2.2) Init console"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L6Xu3Cuhgb7"
      },
      "outputs": [],
      "source": [
        "list_events_test=list()\n",
        "def consoleln(text, color='white'):\n",
        "  global list_events_test, allow_colab_print\n",
        "  list_events_test.append(\"<span style='color:\"+color+\";'>\"+\" \"+text+\"</span><br>\")\n",
        "  if allow_colab_print:\n",
        "    print(text)\n",
        "\n",
        "def console(text, color='white'):\n",
        "  global list_events_test\n",
        "  list_events_test.append(\"<span style='color:\"+color+\";'>\"+\" \"+text+\"</span>\")\n",
        "  if allow_colab_print:\n",
        "    print(text+\" \", end='')\n",
        "\n",
        "def consolesep():\n",
        "  consoleln(\"------------------------------------------------------------------------------------------------------\", 'gray')\n",
        "\n",
        "def consolenext():\n",
        "  console(\"-------------------------------------------------------\", 'cyan')\n",
        "  console(\"New Execution\")\n",
        "  consoleln(\"-----------------------------------------------------\", 'cyan')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz97XM3PnEWe"
      },
      "source": [
        "##2.3) Global vars, Better run this only once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOe4lpbf8P3X"
      },
      "outputs": [],
      "source": [
        "# Deep learning globals\n",
        "w2v_model = Word2Vec()                  # Current w2v model\n",
        "dl_model = Sequential()                 # Current deep learning model (Case Lstm/GRU)\n",
        "dl_model_rev = Sequential()             # Reverse model of dl_model (Case Lstm/GRU)\n",
        "dl_model_bert = SentenceTransformer()   # Current deep learning model (BERT)\n",
        "\n",
        "custom_modeltype = ''                    # User's custom model type 'lstm_gru' <-> 'bert'\n",
        "custom_nn = Sequential()                 # User's custom NN Ar->En (LSTM/GRU)\n",
        "custom_nn_reverse = Sequential()         # User's custom NN En->Ar (LSTM/GRU)\n",
        "custom_nn_bert = SentenceTransformer()   #User's custom NN (BERT)\n",
        "\n",
        "# Datasets\n",
        "data = None                             # STS dataset\n",
        "split_ratio = 80                        # Split between traindata and testdata, it is changeable depending on the model selected\n",
        "user_dataset = None                     # User's instance of STS dataset\n",
        "\n",
        "# Duplication detectors, so we dont have to do the same long operations. BAD IDEA TO MESS WITH THIS GROUPE\n",
        "bert_embeddings_created = False         # Check wether we created BERT's Embeddings\n",
        "bert_embeddings_ar: list()              # Global bert embeddings for dataset sts\n",
        "bert_embeddings_en: list()              # Same as above\n",
        "dataset_loaded = False                  # not yet used\n",
        "current_word2vec_name=''                # Check if we already loaded the same spesific Word2Vec\n",
        "var_radio_dl_model=''                   # Check the selected radio button if we need to load a new DL Model\n",
        "var_radio_dl_model_old=''               # Is used to compare with the current selected DL Model\n",
        "old_dl_model_name=''                    # Same as 'var_radio_dl_model' but is used in coding instead of flask interface\n",
        "custom_nn_model = False                 # Using this to display special interface for testing user's custom model\n",
        "\n",
        "# Misc.\n",
        "maxlength=20                            # We define the minimum 'maxlength' for our models that are already created\n",
        "list_events_test=list()                 # this list will be written in console\n",
        "first_run = True                        # Make sure to reinint console after first main run\n",
        "allow_colab_print = True                # Self-explanatory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwSAAuk8da7k"
      },
      "source": [
        "#3) Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXCoZiLMdgNZ",
        "outputId": "03ea0066-7414-480b-91c8-d75d636ad2d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Montage du 'Google Drive' pour avoir accès au dataset et différents modeès Word2Vec AR-EN , et des diferent modèles entrainés de deelplearning (LSTM, GRU, et BERT)\n",
            "------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Mail:       pfewalidhocine2022@gmail.com\n",
        "# PassWord:   PFEWALIDHOCINE1999\n",
        "\n",
        "drive.mount('/content/drive') \n",
        "os.chdir('/content')\n",
        "\n",
        "console(\"Montage du\")\n",
        "console(\"'Google Drive'\", 'green')\n",
        "console(\"pour avoir accès au dataset et différents modeès\")\n",
        "console(\"Word2Vec AR-EN\", 'pink')\n",
        "console(\", et des diferent modèles entrainés de deelplearning\")\n",
        "consoleln(\"(LSTM, GRU, et BERT)\", 'pink')\n",
        "consolesep()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmZu3TlTgwVu"
      },
      "source": [
        "#4) Load Stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brr63eZ0CsIB"
      },
      "source": [
        "##4.1) Load STS dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dfGLwgehmCC",
        "outputId": "c802696f-fce4-41ea-d21a-e43e7a9aceb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset without shuffling...\n",
            "Fin de lecture de dataset STS.\n",
            "Taille de la list du dataset = 250 lignes\n",
            "Taille du données d'apprentissage =  200 lignes\n",
            "Taille du données de tests =  50 lignes\n",
            "------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Read dataset + split into trainset/testset\n",
        "class DatasetLoader:\n",
        "  def __init__(self):\n",
        "    self.lines_en = list()\n",
        "    self.lines_ar = list()\n",
        "    self.lines_score = list()\n",
        "    self.train_en = list()\n",
        "    self.train_ar = list()\n",
        "    self.train_score = list()\n",
        "    self.test_en = list()\n",
        "    self.test_ar = list()\n",
        "    self.test_score = list()\n",
        "    self.size = 0\n",
        "    self.train_size = 0\n",
        "    self.test_size = 0\n",
        "\n",
        "  def load(self, split_ratio, random = False):\n",
        "    print('Loading dataset ', end='', sep='')\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/dataset.txt\", \"r\") as dataset_file:\n",
        "      for line in dataset_file:\n",
        "        seg = line.split(\"\\t\")\n",
        "        seg[0] = re.sub(r'[^\\w]', ' ', seg[0])\n",
        "        seg[1] = re.sub(r'[^\\w]', ' ', seg[1])\n",
        "        seg[1] = araby.strip_diacritics(seg[1])\n",
        "        self.lines_en.append(seg[0])\n",
        "        self.lines_ar.append(seg[1])\n",
        "        #print(\"taille lines ar\",len(lines_ar))\n",
        "        #print(\"taille lines en\",len(lines_en))\n",
        "    # Read scores\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/dataset_score.txt\", \"r\") as score_file:\n",
        "      for line in score_file:\n",
        "        self.lines_score.append(float(line.strip()))\n",
        "\n",
        "    split_value =  floor(split_ratio*len(self.lines_ar))\n",
        "    if random:\n",
        "      print('with shuffling...')\n",
        "      temp_en, temp_ar, temp_sc= shuffle(self.lines_en,self.lines_ar, self.lines_score)\n",
        "      self.train_en = temp_en[:split_value]\n",
        "      self.train_ar = temp_ar[:split_value]\n",
        "      self.train_score = temp_sc[:split_value]\n",
        "      self.test_en = temp_en[split_value:]\n",
        "      self.test_ar = temp_ar[split_value:]\n",
        "      self.test_score = temp_sc[split_value:]\n",
        "      self.size = len(temp_en)\n",
        "      self.train_size = len(self.train_en)\n",
        "      self.test_size = len(self.test_en)\n",
        "    else:\n",
        "      print('without shuffling...')\n",
        "      self.train_en = self.lines_en[:split_value]\n",
        "      self.train_ar = self.lines_ar[:split_value]\n",
        "      self.train_score = self.lines_score[:split_value]\n",
        "      self.test_en = self.lines_en[split_value:]\n",
        "      self.test_ar = self.lines_ar[split_value:]\n",
        "      self.test_score = self.lines_score[split_value:]\n",
        "      self.size = len(self.lines_en)\n",
        "      self.train_size = len(self.train_en)\n",
        "      self.test_size = len(self.test_en)\n",
        "\n",
        "try:\n",
        "    del data\n",
        "except NameError:\n",
        "  pass\n",
        "\n",
        "#list_events_test.append(\"Debut de lecture du dataset STS avec découpage entre les données training/test = \"+str(split_ratio)+\"%...\")\n",
        "data = DatasetLoader()\n",
        "data.load(split_ratio/100, False)\n",
        "\n",
        "console(\"Fin de lecture de dataset\")\n",
        "consoleln(\"STS.\",'green')\n",
        "\n",
        "console(\"Taille de la list du dataset =\")\n",
        "consoleln(str(data.size)+\" lignes\", 'pink')\n",
        "\n",
        "console(\"Taille du données d\\'apprentissage = \")\n",
        "consoleln(str(data.train_size)+\" lignes\", 'pink')\n",
        "\n",
        "console(\"Taille du données de tests = \")\n",
        "consoleln(str(data.test_size)+\" lignes\", 'pink')\n",
        "\n",
        "consolesep()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAQeE42rdlUQ"
      },
      "source": [
        "##4.2) Gensim Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kbD8d5ZdnO-",
        "outputId": "5bf20458-ce8c-4fe0-9911-508024830cdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargement d'un nouveau modèle Word2Vec AR-EN à partir du Google Drive, cela peut prende un moment...\n",
            "le nom code du modèle est: SkipGram_WordByWord "
          ]
        }
      ],
      "source": [
        "def load_w2v_model(w2v_model_name):\n",
        "  global w2v_model, current_word2vec_name, list_events_test\n",
        "  if current_word2vec_name !=w2v_model_name:\n",
        "    console(\"Chargement d\\'un nouveau modèle\")\n",
        "    console(\"Word2Vec AR-EN\", 'green')\n",
        "    consoleln(\"à partir du Google Drive, cela peut prende un moment...\")\n",
        "    console(\"le nom code du modèle est:\")\n",
        "    console(str(w2v_model_name), \"pink\")\n",
        "    current_word2vec_name = w2v_model_name\n",
        "\n",
        "    os.chdir('/content/drive/My Drive/ArbEngVec gensim/')\n",
        "    if w2v_model_name == 'SkipGram_ParaAlignement':\n",
        "      w2v_model = Word2Vec.load(\"ParallelAlign_Skip/nonshuffle_5window_skipgram_300size.model\")\n",
        "    elif w2v_model_name == 'SkipGram_WordByWord':\n",
        "      w2v_model = Word2Vec.load(\"WordByWord_Skip/wordbywordshuffle_5window_skipgram_300size.model\")\n",
        "    elif w2v_model_name == 'SkipGram_RandomShuffle':\n",
        "      w2v_model = Word2Vec.load(\"RandomShuffle_Skip/randshuffle_5window_skipgram_300size.model\")\n",
        "    elif w2v_model_name == 'CBOW_ParaAlignement':\n",
        "      w2v_model = Word2Vec.load(\"ParallelAlign_CBOW/nonshuffle_5window_cbow_300size.model\")\n",
        "    elif w2v_model_name == 'CBOW_WordByWord':\n",
        "      w2v_model = Word2Vec.load(\"WordByWord_CBOW/wordbywordshuffle_5window_cbow_300size.model\")\n",
        "    elif w2v_model_name == 'CBOW_RandomShuffle':\n",
        "      w2v_model = Word2Vec.load(\"RandomShuffle_CBOW/randshuffle_5window_cbow_300size.model\")\n",
        "    else:\n",
        "      raise Exception(\"Incorrect Word2Vec model name\")\n",
        "      consoleln(\"Le nom du modèle Word2Vec est incorrect!\", 'red')\n",
        "      consoleln(\"Le nouveau modèle Word2Vec à été téléchargé avec succès!\", 'green')\n",
        "  else:\n",
        "    consoleln(\"Le même modèle Word2Vec est déjà chargé, pas besoin de le télécharger à nouveau!\", 'yellow')\n",
        "  os.chdir('/content')\n",
        "  gc.collect()\n",
        "  return\n",
        "\n",
        "load_w2v_model('SkipGram_WordByWord')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qiJbRY4v0ba"
      },
      "source": [
        "##4.3) Load our own trained LSTM/GRU/BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmDRhmp1vwIa"
      },
      "outputs": [],
      "source": [
        "def load_deep_learning_model_pre(dl_model_name):\n",
        "  global dl_model, dl_model_rev, dl_model_bert, bert_embeddings_created, list_events_test, old_dl_model_name\n",
        "  consoleln(\"chargement d'un nouveau modèle de deep learning pré-entrainé par nous...\")\n",
        "  console(\"le nom code du modèle de deep learning est:\")\n",
        "  consoleln(str(dl_model_name), \"pink\")\n",
        "  os.chdir('/content/drive/My Drive/Colab Notebooks/Web/pre_entrainer')\n",
        "\n",
        "  if old_dl_model_name == dl_model_name:\n",
        "    consoleln(\"Le modèle deep learning n'a pas été changé, en garde le modèle ancien...\", 'yellow')\n",
        "    return\n",
        "  else:\n",
        "    consoleln(\"Le modèle deep learning a  été changé, Cela va prendre du temps pour chargé ce que vous avez choisi...\", 'yellow')\n",
        "  \n",
        "  dl_models_list={\n",
        "    # -- LSTM --            \n",
        "    'LSTM_1':           'LSTM_300_RandomGensim_3_couche_500_epoch_50_tanh_batch_1_ar_en_75%.h5', \n",
        "    'LSTM_1rev':        'LSTM_300_RandomGensim_3_couche_500_epoch_50_tanh_batch_1_en_ar_75%.h5', \n",
        "\n",
        "    'LSTM_2':           'LSTM_300_RandomGensim_3_couche_500_epoch_50_tanh_batch_4_ar_en_75%.h5', \n",
        "    'LSTM_2rev':        'LSTM_300_RandomGensim_3_couche_500_epoch_50_tanh_batch_4_en_ar_75%.h5', \n",
        "    \n",
        "    'LSTM_3':           'LSTM_300_1_couche_500_epoch_50_sigmoid_ar_en.h5',\n",
        "    'LSTM_3rev':        'LSTM_300_1_couche_500_epoch_50_sigmoid_en_ar.h5',\n",
        "\n",
        "    # -- Bi. LSTM --\n",
        "    'LSTM_B1':          'Bidi_LStm_2couche_dropout(0.2_0.3)_sigmoid_Epoch250_data70%_ar_en_accuracy24%_.h5',\n",
        "    'LSTM_B1rev':       'Bidi_LStm_2couche_dropout(0.2_0.3)_sigmoid_Epoch250_data70%_en_ar_accuracy23%_.h5',\n",
        "\n",
        "    'LSTM_B2':          'sp_Bid_LSTM300_RandomGensim300__2couche340_epoch300_tanh_dropout_batch_1_ar_en_resultat_18_48%_data_75%.h5',\n",
        "    'LSTM_B2rev':       'sp_Bid_LSTM300_RandomGensim300__2couche340_epoch300_tanh_dropout_batch_1_en_ar_resultat_20%_data_75%.h5',\n",
        "\n",
        "    'LSTM_unused1':     '',\n",
        "    'LSTM_unused1rev':  'Bid_LSTM300_RandomGensim300__2couche340_epoch300_tanh_batch_1_en_ar_resultat_23.5%_data_75%.h5',\n",
        "\n",
        "    # -- GRU --\n",
        "    'GRU_1':            'GRU_300_RandomGensim300_3couche400_epoch750_tanh_batch20_dropout0.1_ar_en_accuracy19%_data75%.h5',  \n",
        "    'GRU_1rev':         'GRU_300_RandomGensim300_3couche400_epoch750_tanh_batch20_dropout0.1_en_ar_accuracy14%_data75%.h5',  \n",
        "\n",
        "    'GRU_2':            'GRU_300_RandomGensim300_1couche500_epoch250_tanh_batch20_ar_en_accuracy17%_data75%.h5',\n",
        "    'GRU_2rev':         'GRU_300_RandomGensim300_1couche500_epoch250_tanh_batch20_en_ar_accuracy18%_data75%.h5',\n",
        "    \n",
        "    'GRU_3':            'GRU_300_RandomGensim300_1couche500_epoch250_tanh_batch20_dropout0.1_ar_en_accuracy14%_data75%.h5',\n",
        "    'GRU_3rev':         'GRU_300_RandomGensim300_1couche500_epoch250_tanh_batch20_dropout0.1_en_ar_accuracy13%_data75%.h5',\n",
        "\n",
        "    # -- Bi. GRU --\n",
        "    'GRU_B1':           'N_bidiGRU_ar_en_accuracy15%.h5',# details dans le  memoire\n",
        "    'GRU_B1rev':        'N_bidiGRU_en_ar_accuracy15_68%.h5',\n",
        "    \n",
        "    'GRU_B2':           'bidi_GRU400_RandomGensim300_3couche400_epoch250_tanh_batch10_dropout(0.2)_ar_en_accuracy19%__data75%.h5',   \n",
        "    'GRU_B2rev':        'bidi_GRU400_RandomGensim300_3couche400_epoch250_tanh_batch10_dropout(0.2)_en_ar_accuracy21%__data75%.h5',\n",
        "    \n",
        "    'GRU_B3':           'Bid_GRU300_RandomGensim300__1couche500_epoch300_tanh_batch_2_ar_en_resultat_20%_data_75%.h5',\n",
        "    'GRU_B3rev':        'Bid_GRU300_RandomGensim300__1couche500_epoch300_tanh_batch_4_en_ar_resultat_21%_data_75%.h5',\n",
        "\n",
        "    'GRU_B4':           'bidi_GRU400_RandomGensim300_2couche400_epoch250_tanh_batch1_ar_en_accuracy21%__data75%.h5',\n",
        "    'GRU_B4rev':        'bidi_GRU400_RandomGensim300_2couche400_epoch250_tanh_batch1_en_ar_accuracy23%_data75%.h5',\n",
        "\n",
        "    'GRU_unusedB5':     'bidi_GRU400_RandomGensim300_2couche400_epoch250_tanh_batch10_ar_en_accuracy20%__data75%.h5',\n",
        "    'GRU_unusedB5rev':  '',\n",
        "\n",
        "    # -- BERT --\n",
        "    'BERT_A':           'modelbert_100ep_batch30.h5',\n",
        "    'BERT_B':           'modelbert_10ep_batch20.h5',\n",
        "    'BERT_C':           'modelbert_50ep_batch40.h5'\n",
        "  }\n",
        "\n",
        "  if dl_model_name.startswith('BERT'):\n",
        "    dl_model_bert = SentenceTransformer(dl_models_list[dl_model_name]+'/')\n",
        "    bert_embeddings_created = False\n",
        "    dl_model = Sequential()\n",
        "    dl_model_rev = Sequential()\n",
        "    gc.collect()\n",
        "    old_dl_model_name = dl_model_name\n",
        "    consoleln(\"le Modèle deep learning a été changé avec success!\", 'green')\n",
        "    return\n",
        "\n",
        "  else: #LSTM/GRU\n",
        "\n",
        "    try:\n",
        "      dl_model = load_model(dl_models_list[dl_model_name])\n",
        "    except OSError:\n",
        "      consoleln('Le Modèle Deep learningne peut pas etre trouvé!!!, Erreur chemin', \"red\")\n",
        "      raise('Model File path not found!!')\n",
        "    except ValueError:\n",
        "      try:\n",
        "        dl_model = load_model(dl_models_list[dl_model_name], custom_objects={\"f1_m\": 'f1_m', 'precision_m':'precision_m', 'recall_m':'recall_m'})\n",
        "      except OSError:\n",
        "        consoleln('Le Modèle Deep learningne peut pas etre trouvé!!!, Erreur chemin', \"red\")\n",
        "        raise('Model File path not found!!')\n",
        "\n",
        "    #Again\n",
        "    try:\n",
        "      dl_model_rev = load_model(dl_models_list[dl_model_name+'rev'])\n",
        "    except OSError:\n",
        "      consoleln('Le Modèle Deep learningne peut pas etre trouvé!!!, Erreur chemin', \"red\")\n",
        "      raise('Model File path not found!!')\n",
        "    except ValueError:\n",
        "      try:\n",
        "        dl_model_rev = load_model(dl_models_list[dl_model_name+'rev'], custom_objects={\"f1_m\": 'f1_m', 'precision_m':'precision_m', 'recall_m':'recall_m'})\n",
        "      except OSError:\n",
        "        consoleln('Le Modèle Deep learningne peut pas etre trouvé!!!, Erreur chemin', \"red\")\n",
        "        raise('Model File path not found!!')\n",
        "    \n",
        "    dl_model_bert = SentenceTransformer()\n",
        "    gc.collect()\n",
        "    old_dl_model_name = dl_model_name\n",
        "    consoleln(\"le Modèle deep learning a été changé avec success!\", 'green')\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFDuWd-6doFH"
      },
      "source": [
        "#5) Utils functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VglWAMwWfMFq"
      },
      "source": [
        "##5.1) First Set: Preprocess STS Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g8J27p9drQu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Returns number of words in the longest sentence exists\n",
        "def get_max_length(text_list):\n",
        "    max_length = 0\n",
        "    for string in text_list:\n",
        "        if len(string.split(\" \")) > max_length:\n",
        "            max_length = len(string.split(\" \"))\n",
        "    return max_length\n",
        "\n",
        "\n",
        "# Replaces some unwanted caracters from sentences\n",
        "def preprocess_string(string:str):\n",
        "  string = string.lower()\n",
        "  string = string.replace('/', ' ').replace('-', ' ').replace('.', ' ')\n",
        "  string = araby.strip_diacritics(string)\n",
        "  return string\n",
        "\n",
        "\n",
        "# Removes escape caracters from words\n",
        "def remove_escape_caracters(word_any:str):\n",
        "  word_any = re.sub('^\\s+', '', word_any)\n",
        "  return re.sub('\\.|,|\\n|،|/|-', '', word_any)\n",
        "\n",
        "\n",
        "# Replaces some caracters in arabic words\n",
        "def rewrite_arabic_word(word_ar:str):\n",
        "  return word_ar.replace('أ', 'ا').replace('إ', 'ا').replace('آ', 'ا').replace('ؤ', 'و').replace('ئ', 'ى').replace('ة', 'ه').replace('ي', 'ى').replace(' ّ', '').replace('اً', 'ا')\n",
        "\n",
        "\n",
        "# Trying to removes some affixes (if present) from a word in case of catch\n",
        "def remove_affix_lv1(word_any:str): \n",
        "  if word_any.startswith('بال') or word_any.startswith('لال') or word_any.startswith('فال')  or word_any.startswith('كال') or word_any.startswith('وال'):\n",
        "    return word_any[1:]\n",
        "  if word_any.endswith('s'):\n",
        "    return word_any[:len(word_any)-1]\n",
        "  if word_any.endswith('ing'):\n",
        "    return word_any[:len(word_any)-3]\n",
        "  return word_any #if nothing resturns as it is\n",
        "\n",
        "\n",
        "# Same thing, for catch n°2\n",
        "def remove_affix_lv2(word_any:str): # in case the word falls in catch lvl 2\n",
        "  if word_any.startswith('بل') or word_any.startswith('لل') or word_any.startswith('فل')  or word_any.startswith('كل') or word_any.startswith('ول'):\n",
        "    return word_any[2:]\n",
        "  if word_any.startswith('ال'):\n",
        "    return word_any[2:]\n",
        "  return word_any\n",
        "\n",
        "\n",
        "# catch n°3\n",
        "def remove_affix_lv3(word_any:str): # in case the word falls in catch lvl 3\n",
        "  liste_1n = ['ا','ي','ت','ك','و','ة','ه']\n",
        "  liste_2n = ['ات','كم','ما','ون','تم','نا','ني','ين','وه','يه','ته','ها','هم','هن','يا','ان','ية','وا', 'ly', 'er']\n",
        "  liste_3n = ['بات','تان','ونن','اته','ونه','تنا','وها','يها','تها','وهم','يون','اها','تين','يين','تهم','هما', 'ful', 'ing']\n",
        "  liste_4n = ['انكم','يتنا','اتنا','يتها','اتها','اتية','اتهم', 'ness', 'able', 'less', 'ment']\n",
        "  liste_5n = ['تموها']\n",
        "\n",
        "  if word_any.endswith('ier') or word_any.endswith('ily'): \n",
        "    return word_any[:len(word_any)-3] + 'y'\n",
        "  if word_any.endswith('iness'):\n",
        "    return word_any[:len(word_any)-4] + 'y'\n",
        "\n",
        "  for endings in liste_1n + liste_2n + liste_3n + liste_4n + liste_5n:\n",
        "    if word_any.endswith(endings):\n",
        "      return word_any[:len(word_any)-len(endings)]\n",
        "  return word_any #if cant do anything\n",
        "\n",
        "\n",
        "# Coding list of string into list of w2v vectors\n",
        "# 1: take one string from the list and tokenizing it into list of words\n",
        "# 2: remove common words\n",
        "# 3: take first word & try to get its w2v vector from a model\n",
        "# 4: if fails, try to modify the word and try again\n",
        "# 5: jump to next word & repeat 3\n",
        "# 6: jump to next string & repeat 1\n",
        "def word2vec_encoding(list_sentences):\n",
        "    global w2v_model, list_events_test\n",
        "    if list_sentences == []:\n",
        "      return []\n",
        "    encoded_list_sentences = list()\n",
        "    encoded_sentence = list()\n",
        "    consoleln(\"Chargement de la liste des mots vides du package NLTK pour la langue spécifique...\")\n",
        "    stop_words_en = set(stopwords.words('english'))\n",
        "    stop_words_ar = set(stopwords.words(\"arabic\"))\n",
        "    consoleln(\"Création d\\'une list des mot vides customisé qui contient des nouveaux mots vides...\")\n",
        "    stop_words_custom =['a', '\\'re', '\\'s', '']\n",
        "\n",
        "    nbr_common_found = 0\n",
        "    nbr_word_non_valid = 0\n",
        "    nbr_tatal_words = 0\n",
        "\n",
        "    consoleln(\"Début de processus de création des embeddings...\")\n",
        "    consoleln(\"Le processus est:\")\n",
        "    consoleln(\"----> 1-: Prendre une phrase de la liste et la faire tokenisé en list des mots\", \"gray\")\n",
        "    consoleln(\"----> 2-: Elimination des mot vides\", \"gray\")\n",
        "    consoleln(\"----> 3-: Prendre un mot de la list des tokens et essayer de trouver son vecteur embeddings en utilisant Word2Vec AR-EN\", \"gray\")\n",
        "    consoleln(\"----> 4-: Si on échoue, on va prendre le mot et lui appliqué un ensemble de modification de niveau 1 (suppression des préfixes et suffixes)\", \"gray\")\n",
        "    consoleln(\"----> 5-: Réessayer de trouver le vecteur du nouveau mot changé\", \"gray\")\n",
        "    consoleln(\"----> 6-: Si on échoue, on va appliqué deux autre ensembles des modifications (niveau 2 et 3) (suppression des préfixes et suffixes qui ne sont pas commune et changement des lettres)\", \"gray\")\n",
        "    consoleln(\"----> 7-: Si on échoue encore, on n\\'a pas pu trouvé le vecteur du mot dans notre word embeddings Word2Vec AR-EN)\", \"gray\")\n",
        "    consoleln(\"----> 8-: Le mot va être oublié et on va prendre le prochain mot de la list des tokens puis repéter (3)\", \"gray\")\n",
        "    consoleln(\"----> 9-: Si on termine la codification des tokens, les embeddings sont mit dans une liste qui represent la phrase courante\", \"gray\")\n",
        "    consoleln(\"----> 10-: Prendre la phrase suivante et repéter (1)\", \"gray\")\n",
        "    consoleln(\"----> 11-: à la fin de codification de tout les phrases, les listes des embeddings généré sont mit dans une liste globale\", \"gray\")\n",
        "\n",
        "    for sentence in list_sentences:\n",
        "        sentence = preprocess_string(sentence)\n",
        "        tokens = word_tokenize(sentence)\n",
        "        encoded_sentence = list()\n",
        "        words = list()\n",
        "        nbr_tatal_words += len(tokens)\n",
        "        for word in tokens:\n",
        "          word = remove_escape_caracters(word)\n",
        "          word = rewrite_arabic_word(word)\n",
        "          if word not in stop_words_en and word not in stop_words_ar and word not in stop_words_custom:\n",
        "            words.append(word)\n",
        "          else:\n",
        "            nbr_common_found += 1\n",
        "        for word in words:\n",
        "          try:\n",
        "            encoded_sentence.append(w2v_model.wv[word]) \n",
        "          except KeyError:\n",
        "            #print('word at lvl1 before [', word, ']', sep='', end='')\n",
        "            word = remove_affix_lv1(word)\n",
        "            #print('\\tword at lvl1  after [', word, ']', sep='')\n",
        "            try:\n",
        "              encoded_sentence.append(w2v_model.wv[word])\n",
        "            except KeyError:\n",
        "              #print('word at lvl2 before [', word, ']', sep='', end='')\n",
        "              word = remove_affix_lv2(word)\n",
        "              #print('\\tword at lvl2 after [', word, ']', sep='')\n",
        "              try:\n",
        "                encoded_sentence.append(w2v_model.wv[word])\n",
        "              except KeyError:\n",
        "                #print('word at lvl3 before [', word, ']', sep='', end='')\n",
        "                word = remove_affix_lv3(word)\n",
        "                #print('\\tword at lvl3 after [', word, ']', sep='')\n",
        "                try:\n",
        "                  encoded_sentence.append(w2v_model.wv[word])\n",
        "                except KeyError:\n",
        "                  nbr_word_non_valid += 1\n",
        "                  #print('invalid word [', word, ']\\n', sep='')\n",
        "        encoded_list_sentences.append(encoded_sentence)\n",
        "    \n",
        "    consoleln(\"Fin du processus, les resultats sont:\", 'green')\n",
        "    console(\"Le nombre des mots totals de la langue courante sont:           \")\n",
        "    consoleln(str(nbr_tatal_words), 'pink')\n",
        "    console(\"Le nombre des mots valides de la langue courante sont:          \")\n",
        "    consoleln(str(nbr_tatal_words - nbr_common_found - nbr_word_non_valid), 'pink')\n",
        "    console(\"Le nombre des mots invalides de la langue courante sont:        \")\n",
        "    consoleln(str(nbr_word_non_valid), 'pink')\n",
        "    console(\"Le nombre des mots vides supprimés de la langue courante sont:  \")\n",
        "    consoleln(str(nbr_common_found), 'pink')\n",
        "    consolesep()\n",
        "    return encoded_list_sentences\n",
        "  \n",
        "# Adding zeors to the end of coded sentences to reach max length, (in the end they'll all have the same length)\n",
        "def add_paddings(encoded_list_sentences, max_length):\n",
        "  padded_encoded_list_sentences = list()\n",
        "  for encoded_sentence in encoded_list_sentences:\n",
        "      zero_padding_cnt = max_length - len(encoded_sentence)\n",
        "      pad = np.zeros((1, 300))\n",
        "      for i in range(zero_padding_cnt):\n",
        "          encoded_sentence = np.concatenate((encoded_sentence, pad), axis=0)\n",
        "      padded_encoded_list_sentences.append(encoded_sentence)\n",
        "  return padded_encoded_list_sentences\n",
        "\n",
        "\n",
        "def preprocess(lines_en, lines_ar):\n",
        "  global maxlength, list_events_test\n",
        "  maxlength_en = get_max_length(lines_en)\n",
        "  maxlength_ar = get_max_length(lines_ar)\n",
        "  console('comparaison de nombre des mots de la plus grande phrase dans la liste des phrase')\n",
        "  console('d\\'anglais ('+str(maxlength_en)+')', 'pink')\n",
        "  console(\"avec celui de la liste \")\n",
        "  console(\"d\\'arabe (\"+str(maxlength_ar)+\")\", 'pink')\n",
        "  console(' Et avec le Mexlength minimum défini par ')\n",
        "  console(\"nous (\"+str(maxlength)+\")\", 'pink')\n",
        "\n",
        "  console('cette comparaison va nous données une idéé sur la taille de paddings qu\\'on va l\\'utilisé après')\n",
        "  maxlen = max(maxlength_en, maxlength_ar, maxlength)\n",
        "  console('Le (MaxLength) a été choisi ('+str(maxlen)+')', 'pink')\n",
        "  consoleln(', Les phrases seront étendues avec des paddings jusqu\\'ils sont tousde même taille.')\n",
        "  consolesep()\n",
        "\n",
        "  console(\"debut de la codification des mots de la list des phrases Anglais pour avoir des embeddings en utilisant\")\n",
        "  console(\"Word2Vec\", \"green\")\n",
        "  consoleln(\"...\")\n",
        "  encoded_en = word2vec_encoding(lines_en)\n",
        "  consoleln(\"fin de la codification de la list d\\'anglais.\", 'green')\n",
        "  consolesep()\n",
        "\n",
        "  console(\"debut de la codification des mots de la list des phrases Arabe pour avoir des embeddings en utilisant\")\n",
        "  console(\"Word2Vec\", \"green\")\n",
        "  consoleln(\"...\")\n",
        "  encoded_ar = word2vec_encoding(lines_ar)\n",
        "  consoleln(\"fin de la codification de la list d\\'arabe.\", 'green')\n",
        "  consolesep()\n",
        "\n",
        "  console(\"Utilisation du\")\n",
        "  console(\"MaxLength\", 'pink')\n",
        "  consoleln(\"pour faire ajouter les paddings au embeddings créés\")\n",
        "  padded_encoded_en = add_paddings(encoded_en, maxlength)\n",
        "  padded_encoded_ar = add_paddings(encoded_ar, maxlength)\n",
        "  consoleln(\"Ajpout des paddings terminé!\", 'green')\n",
        "\n",
        "  embeddings_en = np.array(padded_encoded_en)\n",
        "  embeddings_ar = np.array(padded_encoded_ar)\n",
        "  return embeddings_en, embeddings_ar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfM5QmtEfQDB"
      },
      "source": [
        "##5.2) Second Set: Similarity & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g0BWZoFfS3R"
      },
      "outputs": [],
      "source": [
        "def calc_cos_similarity_lstm_gru(lstm_gru_output, top_n):\n",
        "  global data, list_events_test\n",
        "  cosine_loss = tf.keras.losses.CosineSimilarity(axis = -1) \n",
        "\n",
        "  consoleln(\"Debut de calcul des similarités entre l\\'input de l\\'utilisateur avec le reste du dataset...\")\n",
        "  consoleln(\"La codification du dataset va subir les même prétraitement pour cération des embeddings...\", 'yellow')\n",
        "  dataset_embed_en, dataset_embed_ar = preprocess(data.lines_en, data.lines_ar)\n",
        "  consoleln(\"Fin de la codification des donnés de test\", \"green\")\n",
        "\n",
        "  lstm_gru_output = lstm_gru_output.astype('double')\n",
        "  cos_list, result_list, most_similaire = list(), list(), list()\n",
        "\n",
        "  consoleln(\"Debut de calcul de mésure de similarité Cosinus entre l\\'input avec tout les dataset...\")\n",
        "  for i in range(len(data.lines_ar)):\n",
        "    cos_list.append(abs(cosine_loss(dataset_embed_ar[i], lstm_gru_output).numpy()))\n",
        "  for i in range(len(data.lines_en)):\n",
        "    cos_list.append(abs(cosine_loss(dataset_embed_en[i], lstm_gru_output).numpy()))\n",
        "\n",
        "  most_similaire = sorted(zip(cos_list, data.lines_ar+data.lines_en), reverse = True)\n",
        "  for i in range(top_n):\n",
        "    result_list.append((most_similaire[i][1].strip(), float(\"{:.4f}\".format(most_similaire[i][0]))))\n",
        "    \n",
        "  consoleln(\"Fin de calcul de mésure de similarité\", 'green')\n",
        "  return result_list\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model_keras(user_input, top_n, langue, custom):\n",
        "  global dl_model, dl_model_rev, w2v_model, list_events_test, custom_nn, custom_nn_reverse\n",
        "  if langue=='ar':\n",
        "    consoleln(\"Basculer vers le model qui accespt la langue Arabe comme entrée\",'yellow')\n",
        "    if custom:\n",
        "      temp_model = custom_nn\n",
        "    else:\n",
        "      temp_model = dl_model\n",
        "  else:\n",
        "    consoleln(\"Basculer vers le model qui accespt la langue Anglais comme entrée\",'yellow')\n",
        "    if custom:\n",
        "      temp_model = custom_nn_reverse\n",
        "    else:\n",
        "      temp_model = dl_model_rev\n",
        "\n",
        "  gc.collect()\n",
        "  consoleln(\"Pour fair l'évaluation la données d'entré d\\'utilisateur va être d\\'abord codifié en word embeddings...\")\n",
        "  embedded_user_input, _ = preprocess([user_input],[])\n",
        "  consoleln(\"La codification de la données d\\'utilisateur a été terminé!\", 'green')\n",
        "\n",
        "  consoleln(\"Le modèle de deep learning courant va essayer de predire le resultat en lui passant les embeddings d\\'utilisteur une par une. La prediction va générer les embeddings une par une aussi\")\n",
        "  y = temp_model.predict(embedded_user_input)\n",
        "  consoleln(\"La prediction a été terminé, nous avons l\\'embeddings résultat!\", \"green\")\n",
        "\n",
        "  consoleln(\"Un jeux de test (à ne pas prendre en considiration)\",\"gray\")\n",
        "  consoleln(\"On va essayé de tourves les embeddings proches (de vrai mots) pour les embeddings prédit par le modèle deep learning courant:\",\"gray\")\n",
        "  for i in range(y.shape[1]):\n",
        "    consoleln(\"Le mot proche d\\'embeddings généré n°\"+str(i+1)+\" est: \"+str(w2v_model.wv.most_similar(positive=y[0,[i]], topn=1)),\"gray\")\n",
        "  return calc_cos_similarity_lstm_gru(y, top_n)\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model_bert(user_input, top_n, custom):\n",
        "  from sentence_transformers import SentenceTransformer, util\n",
        "  global dl_model_bert, bert_embeddings_created, bert_embeddings_ar, bert_embeddings_en, custom_nn_bert, data\n",
        "  if custom:\n",
        "    dl_model_bert = custom_nn_bert\n",
        "    gc.collect()\n",
        "    bert_embeddings_created = False\n",
        "\n",
        "  # Compute embedding for both lists\n",
        "  if not bert_embeddings_created or custom:\n",
        "    consoleln(\"Les embeddings du dataset pour le modèle BERT courant ne sont pas créées, on va les créé maintenant\", 'yellow')\n",
        "    \n",
        "    bert_embeddings_ar = dl_model_bert.encode(data.lines_ar, convert_to_tensor=True)\n",
        "    bert_embeddings_en = dl_model_bert.encode(data.lines_en, convert_to_tensor=True)\n",
        "    bert_embeddings_created = True\n",
        "    consoleln(\"Fin de creation des embeddings!\", 'green')\n",
        "    gc.collect()\n",
        "  else:\n",
        "    consoleln(\"Les embeddings du dataset pour le modèle BERT existent deja, pas la peine de les créer à nouveau\", 'yellow')\n",
        "\n",
        "  # Embed user's input  \n",
        "  consoleln(\"Codification du l'entré de l'utilisateur en embeddings\", 'yellow')\n",
        "  bert_user_embeddings = dl_model_bert.encode(user_input, convert_to_tensor=True)\n",
        "\n",
        "  # Compute cosine similarities\n",
        "  consoleln(\"Calcule de la similarité entre les embeddings de dataset avec celui de l'utilisateur\", 'yellow')\n",
        "  cosine_scores_ar = util.cos_sim(bert_embeddings_ar, bert_user_embeddings)\n",
        "  cosine_scores_en = util.cos_sim(bert_embeddings_en, bert_user_embeddings)\n",
        "\n",
        "  # Concat two tensors results ar + en\n",
        "  concated = torch.cat((cosine_scores_ar, cosine_scores_en))\n",
        "\n",
        "  # Sort while keeping in mind swaping the order of dataset lines too\n",
        "  cosine_scores = sorted(zip(concated, data.lines_ar+data.lines_en), reverse = True)\n",
        "  \n",
        "  consoleln(\"Normalisation les valeurs de similartés entre [0,1] et les faire le trié selon l'odre decroissant\", 'yellow')\n",
        "  # Normalizing similarity into [0, 1] instead of [-1, 1] & rewriting evrything into a good looking list\n",
        "  cosine_scores = [(y.strip(), float(\"{:.4f}\".format((x[0].item()+1)/2))) for x,y in cosine_scores]\n",
        "\n",
        "  # Assertion check\n",
        "  for x in cosine_scores:\n",
        "    assert float(x[1])>=0 and float(x[1]) <=1\n",
        "  \n",
        "  return cosine_scores[0:top_n]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6BM3KcbfhhK"
      },
      "source": [
        "##5.3) Third Set: Flask Route Functions Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKarIDs6SvFY"
      },
      "outputs": [],
      "source": [
        "def parse_request(request):\n",
        "  nn_model_type = ''\n",
        "  w2v_model_type = ''\n",
        "  nbr_layers = ''\n",
        "  nbr_in_layer1 = ''\n",
        "  nbr_in_layer2 = ''\n",
        "  nbr_in_layer3 = ''\n",
        "  nbr_in_layer4 = ''\n",
        "  nbr_in_layer5 = ''\n",
        "  dense = ''\n",
        "  weights = ''\n",
        "  dropout = ''\n",
        "  activ_fun = ''\n",
        "  learn_rate = ''\n",
        "  momentum = ''\n",
        "  decay_rate = ''\n",
        "  loss_fun = ''\n",
        "  epochs = ''\n",
        "  batchsize = ''\n",
        "  optim = ''\n",
        "  split_ratio = ''\n",
        "  embeddings_length = ''\n",
        "\n",
        "  nn_model_type = request.form['radgroup2']\n",
        "  if nn_model_type != 'BERT':\n",
        "    w2v_model_type = request.form['radgroup']\n",
        "  if nn_model_type == 'LSTM':\n",
        "    nbr_layers = request.form['nodesnumber_lstm']\n",
        "    nbr_in_layer1 = request.form['lstm_nbr_units_1']\n",
        "    nbr_in_layer2 = request.form['lstm_nbr_units_2']\n",
        "    nbr_in_layer3 = request.form['lstm_nbr_units_3']\n",
        "    nbr_in_layer4 = request.form['lstm_nbr_units_4']\n",
        "    nbr_in_layer5 = request.form['lstm_nbr_units_5']\n",
        "    dense =  request.form['densesnumber_lstm']\n",
        "    weights = request.form['weights_lstm']\n",
        "    dropout = request.form['dropout_lstm']\n",
        "    activ_fun = request.form['activ_fun_lstm']\n",
        "    learn_rate = request.form['Learn_rate_lstm']\n",
        "    momentum = request.form['momentum_lstm']\n",
        "    decay_rate = request.form['decay_lstm']\n",
        "    loss_fun = request.form['loss_fun_lstm']\n",
        "    epochs = request.form['nbrepoch_lstm']\n",
        "    batchsize = request.form['batch_lstm']\n",
        "    optim = request.form['optim_fun_lstm']\n",
        "    split_ratio = request.form['split_ratio_lstm']\n",
        "  elif nn_model_type == 'GRU':\n",
        "    nbr_layers = request.form['nodesnumber_gru']\n",
        "    nbr_in_layer1 = request.form['gru_nbr_units_1']\n",
        "    nbr_in_layer2 = request.form['gru_nbr_units_2']\n",
        "    nbr_in_layer3 = request.form['gru_nbr_units_3']\n",
        "    nbr_in_layer4 = request.form['gru_nbr_units_4']\n",
        "    nbr_in_layer5 = request.form['gru_nbr_units_5']\n",
        "    dense =  request.form['densesnumber_gru']\n",
        "    weights = request.form['weights_gru']\n",
        "    dropout = request.form['dropout_gru']\n",
        "    activ_fun = request.form['activ_fun_gru']\n",
        "    learn_rate = request.form['Learn_rate_gru']\n",
        "    momentum = request.form['momentum_gru']\n",
        "    decay_rate = request.form['decay_gru']\n",
        "    loss_fun = request.form['loss_fun_gru']\n",
        "    epochs = request.form['nbrepoch_gru']\n",
        "    optim = request.form['optim_fun_gru']\n",
        "    batchsize = request.form['batch_gru']\n",
        "    split_ratio = request.form['split_ratio_rgu']\n",
        "  else: # BERT\n",
        "    embeddings_length = request.form['embeddings_lenth_bert']\n",
        "    activ_fun = request.form['actv_func_bert']\n",
        "    loss_fun = request.form['loss_func_bert']\n",
        "    epochs = request.form['nbrepoch_bert']\n",
        "    split_ratio = request.form['split_ratio_bert']\n",
        "    batchsize = request.form['batch_bert']\n",
        "\n",
        "  if nn_model_type == 'LSTM' or nn_model_type == 'GRU':\n",
        "    consoleln('-----------------------------------------------------------------------------------')\n",
        "    consoleln('--------------------------------- Summary -----------------------------------------')\n",
        "    consoleln('-----------------------------------------------------------------------------------')\n",
        "    consoleln('NN model: '+nn_model_type)\n",
        "    consoleln('W2V model: '+w2v_model_type)\n",
        "    consoleln('Nbr Layers: '+nbr_layers)\n",
        "    consoleln('Layer 1: '+nbr_in_layer1)\n",
        "    consoleln('Layer 2: '+nbr_in_layer2)\n",
        "    consoleln('Layer 3: '+nbr_in_layer3)\n",
        "    consoleln('Layer 4: '+nbr_in_layer4)\n",
        "    consoleln('Layer 5: '+nbr_in_layer5)\n",
        "    consoleln('Dense: '+dense)\n",
        "    consoleln('Weights: '+weights)\n",
        "    consoleln('Dropout: '+dropout)\n",
        "    consoleln('Activation: '+activ_fun)\n",
        "    consoleln('Optimizer: '+optim)\n",
        "    consoleln('Learn rate: '+learn_rate)\n",
        "    consoleln('Momentum: '+momentum)\n",
        "    consoleln('Decay rate: '+decay_rate)\n",
        "    consoleln('Loss funct: '+loss_fun)\n",
        "    consoleln('Epochs: '+epochs)\n",
        "    consoleln('split ratio: '+split_ratio+\"%\")\n",
        "    consoleln('Batch size: '+batchsize+\"%\")\n",
        "    consoleln('-----------------------------------------------------------------------------------')\n",
        "    consoleln('-----------------------------------------------------------------------------------')\n",
        "  else:\n",
        "    consoleln('-----------------------------------------------------------------------------------')\n",
        "    consoleln('--------------------------------- Summary -----------------------------------------')\n",
        "    consoleln('-----------------------------------------------------------------------------------')\n",
        "    consoleln('NN model: '+nn_model_type)\n",
        "    consoleln('Embeddings Length: '+embeddings_length)\n",
        "    consoleln('Activation: '+activ_fun)\n",
        "    consoleln('Loss funct: '+loss_fun)\n",
        "    consoleln('Epochs: '+epochs)\n",
        "    consoleln('split ratio: '+split_ratio+\"%\")\n",
        "    consoleln('Batch size: '+batchsize+\"%\")\n",
        "    consoleln('-----------------------------------------------------------------------------------')\n",
        "    consoleln('-----------------------------------------------------------------------------------')\n",
        "  return  nn_model_type,\\\n",
        "          w2v_model_type ,\\\n",
        "          nbr_layers,\\\n",
        "          nbr_in_layer1,\\\n",
        "          nbr_in_layer2,\\\n",
        "          nbr_in_layer3,\\\n",
        "          nbr_in_layer4,\\\n",
        "          nbr_in_layer5,\\\n",
        "          dense,\\\n",
        "          weights,\\\n",
        "          dropout,\\\n",
        "          activ_fun,\\\n",
        "          learn_rate,\\\n",
        "          momentum,\\\n",
        "          decay_rate,\\\n",
        "          loss_fun,\\\n",
        "          epochs,\\\n",
        "          batchsize,\\\n",
        "          optim,\\\n",
        "          split_ratio,\\\n",
        "          embeddings_length\n",
        "\n",
        "def get_keras_loss_fun(loss_fun):\n",
        "  if loss_fun == 'Mean Squared Error':\n",
        "    return tf.keras.losses.MeanSquaredError()\n",
        "  elif loss_fun == 'Mean Absolute Error':\n",
        "    return tf.keras.losses.MeanAbsoluteError()\n",
        "  elif loss_fun == 'Mean Squared Logarithmic Error':\n",
        "    return tf.keras.losses.MeanSquaredLogarithmicError()\n",
        "  elif loss_fun == 'Cosine Similarity Loss':\n",
        "    return tf.keras.losses.CosineSimilarity(axis=1)\n",
        "  elif loss_fun == 'LogCosh Loss':\n",
        "    return tf.keras.losses.LogCosh()\n",
        "\n",
        "def get_keras_optimizer(optim, lr_schedule, momentum) :\n",
        "    if optim == 'SGD':\n",
        "      return tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=float(momentum))\n",
        "    elif optim == 'RMSprop':\n",
        "      return tf.keras.optimizers.RMSprop(learning_rate=lr_schedule, momentum=float(momentum))\n",
        "    elif optim == 'Adam':\n",
        "      return tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "    elif optim == 'Adadelta':\n",
        "      return tf.keras.optimizers.Adadelta(learning_rate=lr_schedule)\n",
        "    elif optim == 'Adagrad':\n",
        "      return tf.keras.optimizers.Adagrad(learning_rate=lr_schedule)\n",
        "    elif optim == 'Adamax':\n",
        "      return tf.keras.optimizers.Adamax(learning_rate=lr_schedule)\n",
        "    elif optim == 'Nadam':\n",
        "      return tf.keras.optimizers.Nadam(learning_rate=lr_schedule)\n",
        "    elif optim == 'Ftrl':\n",
        "      return tf.keras.optimizers.Ftrl(learning_rate=lr_schedule)\n",
        "\n",
        "def get_bert_active_fun(activ_fun):\n",
        "  if activ_fun == 'ReLU':\n",
        "    return nn.ReLU()\n",
        "  elif activ_fun == 'LeakyReLU':\n",
        "    return nn.LeakyReLU()\n",
        "  elif activ_fun == 'Sigmoid':\n",
        "    return nn.Sigmoid()\n",
        "  elif activ_fun == 'Tanh':\n",
        "    return nn.Tanh()\n",
        "  elif activ_fun == 'Softmax':\n",
        "    return nn.Softmax()\n",
        "\n",
        "def get_bert_loss_fun(bert_model, loss_fun):\n",
        "  if loss_fun == 'ContrastiveLoss':\n",
        "    return losses.ContrastiveLoss(model=bert_model)\n",
        "  elif loss_fun == 'CosineSimilarityLoss':\n",
        "    return losses.CosineSimilarityLoss(model=bert_model)\n",
        "  elif loss_fun == 'SoftmaxLoss':\n",
        "    return losses.SoftmaxLoss(model=bert_model)\n",
        "  #\n",
        "  #elif loss_fun == 'MSELoss':\n",
        "  #  return losses.MSELoss(model=bert_model)\n",
        "  #elif loss_fun == 'MarginMSELoss':\n",
        "  #  return losses.MarginMSELoss(model=bert_model)\n",
        "  #elif loss_fun == 'MultipleNegativesRankingLoss':\n",
        "  #  return losses.MultipleNegativesRankingLoss(model=bert_model) # Recommended\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLrXZRI9dswp"
      },
      "source": [
        "#6) Flask App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdNBdLRTK-4c",
        "outputId": "b0d74c59-0f6f-4ca0-f07e-1212912ccb92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Running on http://176d-34-74-209-106.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:04] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:04] \"\u001b[37mGET /static/style5.css HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:04] \"\u001b[37mGET /static/page_principale.css HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:04] \"\u001b[37mGET /static/jquery-3.3.1.slim.min.js HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:04] \"\u001b[37mGET /static/solid.js HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:04] \"\u001b[37mGET /static/logo.png HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:04] \"\u001b[37mGET /static/fontawesome.js HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:04] \"\u001b[37mGET /static/popper.min.js HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:04] \"\u001b[37mGET /static/bootstrap.min.js HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:05] \"\u001b[37mGET /static/img1.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:05] \"\u001b[37mGET /static/img2.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:05] \"\u001b[37mGET /static/img3.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:05] \"\u001b[37mGET /static/img4.webp HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:07] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:20] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:05:20] \"\u001b[37mGET /static/bootstrap.min.css HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:06:07] \"\u001b[37mGET /index6.html HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:06:07] \"\u001b[37mGET /static/bootstrap-toggle.min.css HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:06:07] \"\u001b[37mGET /static/mCustomScrollbar.concat.min.js HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:06:07] \"\u001b[37mGET /static/bootstrap-toggle.min.js HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:06:08] \"\u001b[37mGET /static/img_bg_form.jpg HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------- New Execution -----------------------------------------------------\n",
            "Loading dataset without shuffling...\n",
            "chargement d'un nouveau modèle de deep learning pré-entrainé par nous...\n",
            "le nom code du modèle de deep learning est: LSTM_1\n",
            "Le modèle deep learning n'a pas été changé, en garde le modèle ancien...\n",
            "Basculer vers le model qui accespt la langue Anglais comme entrée\n",
            "Pour fair l'évaluation la données d'entré d'utilisateur va être d'abord codifié en word embeddings...\n",
            "comparaison de nombre des mots de la plus grande phrase dans la liste des phrase d'anglais (15) avec celui de la liste  d'arabe (0)  Et avec le Mexlength minimum défini par  nous (20) cette comparaison va nous données une idéé sur la taille de paddings qu'on va l'utilisé après Le (MaxLength) a été choisi (20) , Les phrases seront étendues avec des paddings jusqu'ils sont tousde même taille.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "debut de la codification des mots de la list des phrases Anglais pour avoir des embeddings en utilisant Word2Vec ...\n",
            "Chargement de la liste des mots vides du package NLTK pour la langue spécifique...\n",
            "Création d'une list des mot vides customisé qui contient des nouveaux mots vides...\n",
            "Début de processus de création des embeddings...\n",
            "Le processus est:\n",
            "----> 1-: Prendre une phrase de la liste et la faire tokenisé en list des mots\n",
            "----> 2-: Elimination des mot vides\n",
            "----> 3-: Prendre un mot de la list des tokens et essayer de trouver son vecteur embeddings en utilisant Word2Vec AR-EN\n",
            "----> 4-: Si on échoue, on va prendre le mot et lui appliqué un ensemble de modification de niveau 1 (suppression des préfixes et suffixes)\n",
            "----> 5-: Réessayer de trouver le vecteur du nouveau mot changé\n",
            "----> 6-: Si on échoue, on va appliqué deux autre ensembles des modifications (niveau 2 et 3) (suppression des préfixes et suffixes qui ne sont pas commune et changement des lettres)\n",
            "----> 7-: Si on échoue encore, on n'a pas pu trouvé le vecteur du mot dans notre word embeddings Word2Vec AR-EN)\n",
            "----> 8-: Le mot va être oublié et on va prendre le prochain mot de la list des tokens puis repéter (3)\n",
            "----> 9-: Si on termine la codification des tokens, les embeddings sont mit dans une liste qui represent la phrase courante\n",
            "----> 10-: Prendre la phrase suivante et repéter (1)\n",
            "----> 11-: à la fin de codification de tout les phrases, les listes des embeddings généré sont mit dans une liste globale\n",
            "Fin du processus, les resultats sont:\n",
            "Le nombre des mots totals de la langue courante sont:            13\n",
            "Le nombre des mots valides de la langue courante sont:           7\n",
            "Le nombre des mots invalides de la langue courante sont:         1\n",
            "Le nombre des mots vides supprimés de la langue courante sont:   5\n",
            "------------------------------------------------------------------------------------------------------\n",
            "fin de la codification de la list d'anglais.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "debut de la codification des mots de la list des phrases Arabe pour avoir des embeddings en utilisant Word2Vec ...\n",
            "fin de la codification de la list d'arabe.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "Utilisation du MaxLength pour faire ajouter les paddings au embeddings créés\n",
            "Ajpout des paddings terminé!\n",
            "La codification de la données d'utilisateur a été terminé!\n",
            "Le modèle de deep learning courant va essayer de predire le resultat en lui passant les embeddings d'utilisteur une par une. La prediction va générer les embeddings une par une aussi\n",
            "La prediction a été terminé, nous avons l'embeddings résultat!\n",
            "Un jeux de test (à ne pas prendre en considiration)\n",
            "On va essayé de tourves les embeddings proches (de vrai mots) pour les embeddings prédit par le modèle deep learning courant:\n",
            "Le mot proche d'embeddings généré n°1 est: [('وزىادتها', 0.2140716016292572)]\n",
            "Le mot proche d'embeddings généré n°2 est: [('المروعىن', 0.24718084931373596)]\n",
            "Le mot proche d'embeddings généré n°3 est: [('خلىفتها', 0.23563110828399658)]\n",
            "Le mot proche d'embeddings généré n°4 est: [('المعىنىن', 0.22892862558364868)]\n",
            "Le mot proche d'embeddings généré n°5 est: [('triad', 0.24185678362846375)]\n",
            "Le mot proche d'embeddings généré n°6 est: [('باوراق', 0.2389785349369049)]\n",
            "Le mot proche d'embeddings généré n°7 est: [('والمعدل', 0.26252859830856323)]\n",
            "Le mot proche d'embeddings généré n°8 est: [('وللاهداف', 0.28806838393211365)]\n",
            "Le mot proche d'embeddings généré n°9 est: [('المبادلات', 0.3111306428909302)]\n",
            "Le mot proche d'embeddings généré n°10 est: [('فمكانته', 0.2987905740737915)]\n",
            "Le mot proche d'embeddings généré n°11 est: [('شىم', 0.29921019077301025)]\n",
            "Le mot proche d'embeddings généré n°12 est: [('miinisters', 0.2888947129249573)]\n",
            "Le mot proche d'embeddings généré n°13 est: [('شىم', 0.29268696904182434)]\n",
            "Le mot proche d'embeddings généré n°14 est: [('كار', 0.28551042079925537)]\n",
            "Le mot proche d'embeddings généré n°15 est: [('دارسىن', 0.2943931818008423)]\n",
            "Le mot proche d'embeddings généré n°16 est: [('اخصاء', 0.27837103605270386)]\n",
            "Le mot proche d'embeddings généré n°17 est: [('دارسىن', 0.28986483812332153)]\n",
            "Le mot proche d'embeddings généré n°18 est: [('بتقصىر', 0.2864667773246765)]\n",
            "Le mot proche d'embeddings généré n°19 est: [('eipa', 0.28003770112991333)]\n",
            "Le mot proche d'embeddings généré n°20 est: [('سىرحل', 0.3271816372871399)]\n",
            "Debut de calcul des similarités entre l'input de l'utilisateur avec le reste du dataset...\n",
            "La codification du dataset va subir les même prétraitement pour cération des embeddings...\n",
            "comparaison de nombre des mots de la plus grande phrase dans la liste des phrase d'anglais (20) avec celui de la liste  d'arabe (19)  Et avec le Mexlength minimum défini par  nous (20) cette comparaison va nous données une idéé sur la taille de paddings qu'on va l'utilisé après Le (MaxLength) a été choisi (20) , Les phrases seront étendues avec des paddings jusqu'ils sont tousde même taille.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "debut de la codification des mots de la list des phrases Anglais pour avoir des embeddings en utilisant Word2Vec ...\n",
            "Chargement de la liste des mots vides du package NLTK pour la langue spécifique...\n",
            "Création d'une list des mot vides customisé qui contient des nouveaux mots vides...\n",
            "Début de processus de création des embeddings...\n",
            "Le processus est:\n",
            "----> 1-: Prendre une phrase de la liste et la faire tokenisé en list des mots\n",
            "----> 2-: Elimination des mot vides\n",
            "----> 3-: Prendre un mot de la list des tokens et essayer de trouver son vecteur embeddings en utilisant Word2Vec AR-EN\n",
            "----> 4-: Si on échoue, on va prendre le mot et lui appliqué un ensemble de modification de niveau 1 (suppression des préfixes et suffixes)\n",
            "----> 5-: Réessayer de trouver le vecteur du nouveau mot changé\n",
            "----> 6-: Si on échoue, on va appliqué deux autre ensembles des modifications (niveau 2 et 3) (suppression des préfixes et suffixes qui ne sont pas commune et changement des lettres)\n",
            "----> 7-: Si on échoue encore, on n'a pas pu trouvé le vecteur du mot dans notre word embeddings Word2Vec AR-EN)\n",
            "----> 8-: Le mot va être oublié et on va prendre le prochain mot de la list des tokens puis repéter (3)\n",
            "----> 9-: Si on termine la codification des tokens, les embeddings sont mit dans une liste qui represent la phrase courante\n",
            "----> 10-: Prendre la phrase suivante et repéter (1)\n",
            "----> 11-: à la fin de codification de tout les phrases, les listes des embeddings généré sont mit dans une liste globale\n",
            "Fin du processus, les resultats sont:\n",
            "Le nombre des mots totals de la langue courante sont:            2103\n",
            "Le nombre des mots valides de la langue courante sont:           1157\n",
            "Le nombre des mots invalides de la langue courante sont:         12\n",
            "Le nombre des mots vides supprimés de la langue courante sont:   934\n",
            "------------------------------------------------------------------------------------------------------\n",
            "fin de la codification de la list d'anglais.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "debut de la codification des mots de la list des phrases Arabe pour avoir des embeddings en utilisant Word2Vec ...\n",
            "Chargement de la liste des mots vides du package NLTK pour la langue spécifique...\n",
            "Création d'une list des mot vides customisé qui contient des nouveaux mots vides...\n",
            "Début de processus de création des embeddings...\n",
            "Le processus est:\n",
            "----> 1-: Prendre une phrase de la liste et la faire tokenisé en list des mots\n",
            "----> 2-: Elimination des mot vides\n",
            "----> 3-: Prendre un mot de la list des tokens et essayer de trouver son vecteur embeddings en utilisant Word2Vec AR-EN\n",
            "----> 4-: Si on échoue, on va prendre le mot et lui appliqué un ensemble de modification de niveau 1 (suppression des préfixes et suffixes)\n",
            "----> 5-: Réessayer de trouver le vecteur du nouveau mot changé\n",
            "----> 6-: Si on échoue, on va appliqué deux autre ensembles des modifications (niveau 2 et 3) (suppression des préfixes et suffixes qui ne sont pas commune et changement des lettres)\n",
            "----> 7-: Si on échoue encore, on n'a pas pu trouvé le vecteur du mot dans notre word embeddings Word2Vec AR-EN)\n",
            "----> 8-: Le mot va être oublié et on va prendre le prochain mot de la list des tokens puis repéter (3)\n",
            "----> 9-: Si on termine la codification des tokens, les embeddings sont mit dans une liste qui represent la phrase courante\n",
            "----> 10-: Prendre la phrase suivante et repéter (1)\n",
            "----> 11-: à la fin de codification de tout les phrases, les listes des embeddings généré sont mit dans une liste globale\n",
            "Fin du processus, les resultats sont:\n",
            "Le nombre des mots totals de la langue courante sont:            1489\n",
            "Le nombre des mots valides de la langue courante sont:           1288\n",
            "Le nombre des mots invalides de la langue courante sont:         36\n",
            "Le nombre des mots vides supprimés de la langue courante sont:   165\n",
            "------------------------------------------------------------------------------------------------------\n",
            "fin de la codification de la list d'arabe.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "Utilisation du MaxLength pour faire ajouter les paddings au embeddings créés\n",
            "Ajpout des paddings terminé!\n",
            "Fin de la codification des donnés de test\n",
            "Debut de calcul de mésure de similarité Cosinus entre l'input avec tout les dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:10:27] \"\u001b[37mPOST /submit_test HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fin de calcul de mésure de similarité\n",
            "------------------------------------------------------- New Execution -----------------------------------------------------\n",
            "Loading dataset without shuffling...\n",
            "chargement d'un nouveau modèle de deep learning pré-entrainé par nous...\n",
            "le nom code du modèle de deep learning est: LSTM_B1\n",
            "Le modèle deep learning a  été changé, Cela va prendre du temps pour chargé ce que vous avez choisi...\n",
            "le Modèle deep learning a été changé avec success!\n",
            "Basculer vers le model qui accespt la langue Anglais comme entrée\n",
            "Pour fair l'évaluation la données d'entré d'utilisateur va être d'abord codifié en word embeddings...\n",
            "comparaison de nombre des mots de la plus grande phrase dans la liste des phrase d'anglais (15) avec celui de la liste  d'arabe (0)  Et avec le Mexlength minimum défini par  nous (20) cette comparaison va nous données une idéé sur la taille de paddings qu'on va l'utilisé après Le (MaxLength) a été choisi (20) , Les phrases seront étendues avec des paddings jusqu'ils sont tousde même taille.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "debut de la codification des mots de la list des phrases Anglais pour avoir des embeddings en utilisant Word2Vec ...\n",
            "Chargement de la liste des mots vides du package NLTK pour la langue spécifique...\n",
            "Création d'une list des mot vides customisé qui contient des nouveaux mots vides...\n",
            "Début de processus de création des embeddings...\n",
            "Le processus est:\n",
            "----> 1-: Prendre une phrase de la liste et la faire tokenisé en list des mots\n",
            "----> 2-: Elimination des mot vides\n",
            "----> 3-: Prendre un mot de la list des tokens et essayer de trouver son vecteur embeddings en utilisant Word2Vec AR-EN\n",
            "----> 4-: Si on échoue, on va prendre le mot et lui appliqué un ensemble de modification de niveau 1 (suppression des préfixes et suffixes)\n",
            "----> 5-: Réessayer de trouver le vecteur du nouveau mot changé\n",
            "----> 6-: Si on échoue, on va appliqué deux autre ensembles des modifications (niveau 2 et 3) (suppression des préfixes et suffixes qui ne sont pas commune et changement des lettres)\n",
            "----> 7-: Si on échoue encore, on n'a pas pu trouvé le vecteur du mot dans notre word embeddings Word2Vec AR-EN)\n",
            "----> 8-: Le mot va être oublié et on va prendre le prochain mot de la list des tokens puis repéter (3)\n",
            "----> 9-: Si on termine la codification des tokens, les embeddings sont mit dans une liste qui represent la phrase courante\n",
            "----> 10-: Prendre la phrase suivante et repéter (1)\n",
            "----> 11-: à la fin de codification de tout les phrases, les listes des embeddings généré sont mit dans une liste globale\n",
            "Fin du processus, les resultats sont:\n",
            "Le nombre des mots totals de la langue courante sont:            13\n",
            "Le nombre des mots valides de la langue courante sont:           7\n",
            "Le nombre des mots invalides de la langue courante sont:         1\n",
            "Le nombre des mots vides supprimés de la langue courante sont:   5\n",
            "------------------------------------------------------------------------------------------------------\n",
            "fin de la codification de la list d'anglais.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "debut de la codification des mots de la list des phrases Arabe pour avoir des embeddings en utilisant Word2Vec ...\n",
            "fin de la codification de la list d'arabe.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "Utilisation du MaxLength pour faire ajouter les paddings au embeddings créés\n",
            "Ajpout des paddings terminé!\n",
            "La codification de la données d'utilisateur a été terminé!\n",
            "Le modèle de deep learning courant va essayer de predire le resultat en lui passant les embeddings d'utilisteur une par une. La prediction va générer les embeddings une par une aussi\n",
            "La prediction a été terminé, nous avons l'embeddings résultat!\n",
            "Un jeux de test (à ne pas prendre en considiration)\n",
            "On va essayé de tourves les embeddings proches (de vrai mots) pour les embeddings prédit par le modèle deep learning courant:\n",
            "Le mot proche d'embeddings généré n°1 est: [('salamibila', 0.7201825976371765)]\n",
            "Le mot proche d'embeddings généré n°2 est: [('قمىصا', 0.8956049680709839)]\n",
            "Le mot proche d'embeddings généré n°3 est: [('bossembele', 0.8429966568946838)]\n",
            "Le mot proche d'embeddings généré n°4 est: [('parpottas', 0.5993813276290894)]\n",
            "Le mot proche d'embeddings généré n°5 est: [('ولعاءداتهم', 0.8000943660736084)]\n",
            "Le mot proche d'embeddings généré n°6 est: [('ازرق', 0.6996115446090698)]\n",
            "Le mot proche d'embeddings généré n°7 est: [('ىجرى', 0.6505464315414429)]\n",
            "Le mot proche d'embeddings généré n°8 est: [('لاستقواء', 0.7641249895095825)]\n",
            "Le mot proche d'embeddings généré n°9 est: [('ومزبات', 0.6100447177886963)]\n",
            "Le mot proche d'embeddings généré n°10 est: [('ىجرى', 0.2734309136867523)]\n",
            "Le mot proche d'embeddings généré n°11 est: [('ىجرى', 0.2702309489250183)]\n",
            "Le mot proche d'embeddings généré n°12 est: [('ىجرى', 0.2692806124687195)]\n",
            "Le mot proche d'embeddings généré n°13 est: [('ىجرى', 0.26924121379852295)]\n",
            "Le mot proche d'embeddings généré n°14 est: [('ىجرى', 0.2690885365009308)]\n",
            "Le mot proche d'embeddings généré n°15 est: [('ىجرى', 0.2690088748931885)]\n",
            "Le mot proche d'embeddings généré n°16 est: [('ىجرى', 0.26938143372535706)]\n",
            "Le mot proche d'embeddings généré n°17 est: [('ىجرى', 0.2692539393901825)]\n",
            "Le mot proche d'embeddings généré n°18 est: [('ىجرى', 0.26915326714515686)]\n",
            "Le mot proche d'embeddings généré n°19 est: [('ىجرى', 0.27024513483047485)]\n",
            "Le mot proche d'embeddings généré n°20 est: [('ىجرى', 0.2692369222640991)]\n",
            "Debut de calcul des similarités entre l'input de l'utilisateur avec le reste du dataset...\n",
            "La codification du dataset va subir les même prétraitement pour cération des embeddings...\n",
            "comparaison de nombre des mots de la plus grande phrase dans la liste des phrase d'anglais (20) avec celui de la liste  d'arabe (19)  Et avec le Mexlength minimum défini par  nous (20) cette comparaison va nous données une idéé sur la taille de paddings qu'on va l'utilisé après Le (MaxLength) a été choisi (20) , Les phrases seront étendues avec des paddings jusqu'ils sont tousde même taille.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "debut de la codification des mots de la list des phrases Anglais pour avoir des embeddings en utilisant Word2Vec ...\n",
            "Chargement de la liste des mots vides du package NLTK pour la langue spécifique...\n",
            "Création d'une list des mot vides customisé qui contient des nouveaux mots vides...\n",
            "Début de processus de création des embeddings...\n",
            "Le processus est:\n",
            "----> 1-: Prendre une phrase de la liste et la faire tokenisé en list des mots\n",
            "----> 2-: Elimination des mot vides\n",
            "----> 3-: Prendre un mot de la list des tokens et essayer de trouver son vecteur embeddings en utilisant Word2Vec AR-EN\n",
            "----> 4-: Si on échoue, on va prendre le mot et lui appliqué un ensemble de modification de niveau 1 (suppression des préfixes et suffixes)\n",
            "----> 5-: Réessayer de trouver le vecteur du nouveau mot changé\n",
            "----> 6-: Si on échoue, on va appliqué deux autre ensembles des modifications (niveau 2 et 3) (suppression des préfixes et suffixes qui ne sont pas commune et changement des lettres)\n",
            "----> 7-: Si on échoue encore, on n'a pas pu trouvé le vecteur du mot dans notre word embeddings Word2Vec AR-EN)\n",
            "----> 8-: Le mot va être oublié et on va prendre le prochain mot de la list des tokens puis repéter (3)\n",
            "----> 9-: Si on termine la codification des tokens, les embeddings sont mit dans une liste qui represent la phrase courante\n",
            "----> 10-: Prendre la phrase suivante et repéter (1)\n",
            "----> 11-: à la fin de codification de tout les phrases, les listes des embeddings généré sont mit dans une liste globale\n",
            "Fin du processus, les resultats sont:\n",
            "Le nombre des mots totals de la langue courante sont:            2103\n",
            "Le nombre des mots valides de la langue courante sont:           1157\n",
            "Le nombre des mots invalides de la langue courante sont:         12\n",
            "Le nombre des mots vides supprimés de la langue courante sont:   934\n",
            "------------------------------------------------------------------------------------------------------\n",
            "fin de la codification de la list d'anglais.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "debut de la codification des mots de la list des phrases Arabe pour avoir des embeddings en utilisant Word2Vec ...\n",
            "Chargement de la liste des mots vides du package NLTK pour la langue spécifique...\n",
            "Création d'une list des mot vides customisé qui contient des nouveaux mots vides...\n",
            "Début de processus de création des embeddings...\n",
            "Le processus est:\n",
            "----> 1-: Prendre une phrase de la liste et la faire tokenisé en list des mots\n",
            "----> 2-: Elimination des mot vides\n",
            "----> 3-: Prendre un mot de la list des tokens et essayer de trouver son vecteur embeddings en utilisant Word2Vec AR-EN\n",
            "----> 4-: Si on échoue, on va prendre le mot et lui appliqué un ensemble de modification de niveau 1 (suppression des préfixes et suffixes)\n",
            "----> 5-: Réessayer de trouver le vecteur du nouveau mot changé\n",
            "----> 6-: Si on échoue, on va appliqué deux autre ensembles des modifications (niveau 2 et 3) (suppression des préfixes et suffixes qui ne sont pas commune et changement des lettres)\n",
            "----> 7-: Si on échoue encore, on n'a pas pu trouvé le vecteur du mot dans notre word embeddings Word2Vec AR-EN)\n",
            "----> 8-: Le mot va être oublié et on va prendre le prochain mot de la list des tokens puis repéter (3)\n",
            "----> 9-: Si on termine la codification des tokens, les embeddings sont mit dans une liste qui represent la phrase courante\n",
            "----> 10-: Prendre la phrase suivante et repéter (1)\n",
            "----> 11-: à la fin de codification de tout les phrases, les listes des embeddings généré sont mit dans une liste globale\n",
            "Fin du processus, les resultats sont:\n",
            "Le nombre des mots totals de la langue courante sont:            1489\n",
            "Le nombre des mots valides de la langue courante sont:           1288\n",
            "Le nombre des mots invalides de la langue courante sont:         36\n",
            "Le nombre des mots vides supprimés de la langue courante sont:   165\n",
            "------------------------------------------------------------------------------------------------------\n",
            "fin de la codification de la list d'arabe.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "Utilisation du MaxLength pour faire ajouter les paddings au embeddings créés\n",
            "Ajpout des paddings terminé!\n",
            "Fin de la codification des donnés de test\n",
            "Debut de calcul de mésure de similarité Cosinus entre l'input avec tout les dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:13:39] \"\u001b[37mPOST /submit_test HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fin de calcul de mésure de similarité\n",
            "------------------------------------------------------- New Execution -----------------------------------------------------\n",
            "Loading dataset without shuffling...\n",
            "chargement d'un nouveau modèle de deep learning pré-entrainé par nous...\n",
            "le nom code du modèle de deep learning est: GRU_B2\n",
            "Le modèle deep learning a  été changé, Cela va prendre du temps pour chargé ce que vous avez choisi...\n",
            "le Modèle deep learning a été changé avec success!\n",
            "Basculer vers le model qui accespt la langue Anglais comme entrée\n",
            "Pour fair l'évaluation la données d'entré d'utilisateur va être d'abord codifié en word embeddings...\n",
            "comparaison de nombre des mots de la plus grande phrase dans la liste des phrase d'anglais (15) avec celui de la liste  d'arabe (0)  Et avec le Mexlength minimum défini par  nous (20) cette comparaison va nous données une idéé sur la taille de paddings qu'on va l'utilisé après Le (MaxLength) a été choisi (20) , Les phrases seront étendues avec des paddings jusqu'ils sont tousde même taille.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "debut de la codification des mots de la list des phrases Anglais pour avoir des embeddings en utilisant Word2Vec ...\n",
            "Chargement de la liste des mots vides du package NLTK pour la langue spécifique...\n",
            "Création d'une list des mot vides customisé qui contient des nouveaux mots vides...\n",
            "Début de processus de création des embeddings...\n",
            "Le processus est:\n",
            "----> 1-: Prendre une phrase de la liste et la faire tokenisé en list des mots\n",
            "----> 2-: Elimination des mot vides\n",
            "----> 3-: Prendre un mot de la list des tokens et essayer de trouver son vecteur embeddings en utilisant Word2Vec AR-EN\n",
            "----> 4-: Si on échoue, on va prendre le mot et lui appliqué un ensemble de modification de niveau 1 (suppression des préfixes et suffixes)\n",
            "----> 5-: Réessayer de trouver le vecteur du nouveau mot changé\n",
            "----> 6-: Si on échoue, on va appliqué deux autre ensembles des modifications (niveau 2 et 3) (suppression des préfixes et suffixes qui ne sont pas commune et changement des lettres)\n",
            "----> 7-: Si on échoue encore, on n'a pas pu trouvé le vecteur du mot dans notre word embeddings Word2Vec AR-EN)\n",
            "----> 8-: Le mot va être oublié et on va prendre le prochain mot de la list des tokens puis repéter (3)\n",
            "----> 9-: Si on termine la codification des tokens, les embeddings sont mit dans une liste qui represent la phrase courante\n",
            "----> 10-: Prendre la phrase suivante et repéter (1)\n",
            "----> 11-: à la fin de codification de tout les phrases, les listes des embeddings généré sont mit dans une liste globale\n",
            "Fin du processus, les resultats sont:\n",
            "Le nombre des mots totals de la langue courante sont:            13\n",
            "Le nombre des mots valides de la langue courante sont:           7\n",
            "Le nombre des mots invalides de la langue courante sont:         1\n",
            "Le nombre des mots vides supprimés de la langue courante sont:   5\n",
            "------------------------------------------------------------------------------------------------------\n",
            "fin de la codification de la list d'anglais.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "debut de la codification des mots de la list des phrases Arabe pour avoir des embeddings en utilisant Word2Vec ...\n",
            "fin de la codification de la list d'arabe.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "Utilisation du MaxLength pour faire ajouter les paddings au embeddings créés\n",
            "Ajpout des paddings terminé!\n",
            "La codification de la données d'utilisateur a été terminé!\n",
            "Le modèle de deep learning courant va essayer de predire le resultat en lui passant les embeddings d'utilisteur une par une. La prediction va générer les embeddings une par une aussi\n",
            "La prediction a été terminé, nous avons l'embeddings résultat!\n",
            "Un jeux de test (à ne pas prendre en considiration)\n",
            "On va essayé de tourves les embeddings proches (de vrai mots) pour les embeddings prédit par le modèle deep learning courant:\n",
            "Le mot proche d'embeddings généré n°1 est: [('للتجاوزات', 0.24123623967170715)]\n",
            "Le mot proche d'embeddings généré n°2 est: [('والمحدد', 0.22168995440006256)]\n",
            "Le mot proche d'embeddings généré n°3 est: [('suggest', 0.22098781168460846)]\n",
            "Le mot proche d'embeddings généré n°4 est: [('بالاباده', 0.2565683424472809)]\n",
            "Le mot proche d'embeddings généré n°5 est: [('غجر', 0.27475181221961975)]\n",
            "Le mot proche d'embeddings généré n°6 est: [('البستانىه', 0.29737675189971924)]\n",
            "Le mot proche d'embeddings généré n°7 est: [('الجامد', 0.27129918336868286)]\n",
            "Le mot proche d'embeddings généré n°8 est: [('مختلا', 0.27830544114112854)]\n",
            "Le mot proche d'embeddings généré n°9 est: [('تحاشىا', 0.2568737268447876)]\n",
            "Le mot proche d'embeddings généré n°10 est: [('للتسلسل', 0.24983903765678406)]\n",
            "Le mot proche d'embeddings généré n°11 est: [('للافلاس', 0.23899193108081818)]\n",
            "Le mot proche d'embeddings généré n°12 est: [('workouts', 0.24433669447898865)]\n",
            "Le mot proche d'embeddings généré n°13 est: [('workouts', 0.25390034914016724)]\n",
            "Le mot proche d'embeddings généré n°14 est: [('arbitrary', 0.23467782139778137)]\n",
            "Le mot proche d'embeddings généré n°15 est: [('arbitrary', 0.24318140745162964)]\n",
            "Le mot proche d'embeddings généré n°16 est: [('لكاسبى', 0.2519671320915222)]\n",
            "Le mot proche d'embeddings généré n°17 est: [('لكاسبى', 0.2574169635772705)]\n",
            "Le mot proche d'embeddings généré n°18 est: [('لكاسبى', 0.24324306845664978)]\n",
            "Le mot proche d'embeddings généré n°19 est: [('لفءه', 0.23860695958137512)]\n",
            "Le mot proche d'embeddings généré n°20 est: [('لفءه', 0.23218101263046265)]\n",
            "Debut de calcul des similarités entre l'input de l'utilisateur avec le reste du dataset...\n",
            "La codification du dataset va subir les même prétraitement pour cération des embeddings...\n",
            "comparaison de nombre des mots de la plus grande phrase dans la liste des phrase d'anglais (20) avec celui de la liste  d'arabe (19)  Et avec le Mexlength minimum défini par  nous (20) cette comparaison va nous données une idéé sur la taille de paddings qu'on va l'utilisé après Le (MaxLength) a été choisi (20) , Les phrases seront étendues avec des paddings jusqu'ils sont tousde même taille.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "debut de la codification des mots de la list des phrases Anglais pour avoir des embeddings en utilisant Word2Vec ...\n",
            "Chargement de la liste des mots vides du package NLTK pour la langue spécifique...\n",
            "Création d'une list des mot vides customisé qui contient des nouveaux mots vides...\n",
            "Début de processus de création des embeddings...\n",
            "Le processus est:\n",
            "----> 1-: Prendre une phrase de la liste et la faire tokenisé en list des mots\n",
            "----> 2-: Elimination des mot vides\n",
            "----> 3-: Prendre un mot de la list des tokens et essayer de trouver son vecteur embeddings en utilisant Word2Vec AR-EN\n",
            "----> 4-: Si on échoue, on va prendre le mot et lui appliqué un ensemble de modification de niveau 1 (suppression des préfixes et suffixes)\n",
            "----> 5-: Réessayer de trouver le vecteur du nouveau mot changé\n",
            "----> 6-: Si on échoue, on va appliqué deux autre ensembles des modifications (niveau 2 et 3) (suppression des préfixes et suffixes qui ne sont pas commune et changement des lettres)\n",
            "----> 7-: Si on échoue encore, on n'a pas pu trouvé le vecteur du mot dans notre word embeddings Word2Vec AR-EN)\n",
            "----> 8-: Le mot va être oublié et on va prendre le prochain mot de la list des tokens puis repéter (3)\n",
            "----> 9-: Si on termine la codification des tokens, les embeddings sont mit dans une liste qui represent la phrase courante\n",
            "----> 10-: Prendre la phrase suivante et repéter (1)\n",
            "----> 11-: à la fin de codification de tout les phrases, les listes des embeddings généré sont mit dans une liste globale\n",
            "Fin du processus, les resultats sont:\n",
            "Le nombre des mots totals de la langue courante sont:            2103\n",
            "Le nombre des mots valides de la langue courante sont:           1157\n",
            "Le nombre des mots invalides de la langue courante sont:         12\n",
            "Le nombre des mots vides supprimés de la langue courante sont:   934\n",
            "------------------------------------------------------------------------------------------------------\n",
            "fin de la codification de la list d'anglais.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "debut de la codification des mots de la list des phrases Arabe pour avoir des embeddings en utilisant Word2Vec ...\n",
            "Chargement de la liste des mots vides du package NLTK pour la langue spécifique...\n",
            "Création d'une list des mot vides customisé qui contient des nouveaux mots vides...\n",
            "Début de processus de création des embeddings...\n",
            "Le processus est:\n",
            "----> 1-: Prendre une phrase de la liste et la faire tokenisé en list des mots\n",
            "----> 2-: Elimination des mot vides\n",
            "----> 3-: Prendre un mot de la list des tokens et essayer de trouver son vecteur embeddings en utilisant Word2Vec AR-EN\n",
            "----> 4-: Si on échoue, on va prendre le mot et lui appliqué un ensemble de modification de niveau 1 (suppression des préfixes et suffixes)\n",
            "----> 5-: Réessayer de trouver le vecteur du nouveau mot changé\n",
            "----> 6-: Si on échoue, on va appliqué deux autre ensembles des modifications (niveau 2 et 3) (suppression des préfixes et suffixes qui ne sont pas commune et changement des lettres)\n",
            "----> 7-: Si on échoue encore, on n'a pas pu trouvé le vecteur du mot dans notre word embeddings Word2Vec AR-EN)\n",
            "----> 8-: Le mot va être oublié et on va prendre le prochain mot de la list des tokens puis repéter (3)\n",
            "----> 9-: Si on termine la codification des tokens, les embeddings sont mit dans une liste qui represent la phrase courante\n",
            "----> 10-: Prendre la phrase suivante et repéter (1)\n",
            "----> 11-: à la fin de codification de tout les phrases, les listes des embeddings généré sont mit dans une liste globale\n",
            "Fin du processus, les resultats sont:\n",
            "Le nombre des mots totals de la langue courante sont:            1489\n",
            "Le nombre des mots valides de la langue courante sont:           1288\n",
            "Le nombre des mots invalides de la langue courante sont:         36\n",
            "Le nombre des mots vides supprimés de la langue courante sont:   165\n",
            "------------------------------------------------------------------------------------------------------\n",
            "fin de la codification de la list d'arabe.\n",
            "------------------------------------------------------------------------------------------------------\n",
            "Utilisation du MaxLength pour faire ajouter les paddings au embeddings créés\n",
            "Ajpout des paddings terminé!\n",
            "Fin de la codification des donnés de test\n",
            "Debut de calcul de mésure de similarité Cosinus entre l'input avec tout les dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:14:56] \"\u001b[37mPOST /submit_test HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fin de calcul de mésure de similarité\n",
            "------------------------------------------------------- New Execution -----------------------------------------------------\n",
            "Loading dataset without shuffling...\n",
            "chargement d'un nouveau modèle de deep learning pré-entrainé par nous...\n",
            "le nom code du modèle de deep learning est: BERT_A\n",
            "Le modèle deep learning a  été changé, Cela va prendre du temps pour chargé ce que vous avez choisi...\n",
            "le Modèle deep learning a été changé avec success!\n",
            "Les embeddings du dataset pour le modèle BERT courant ne sont pas créées, on va les créé maintenant\n",
            "Fin de creation des embeddings!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:18:33] \"\u001b[37mPOST /submit_test HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Codification du l'entré de l'utilisateur en embeddings\n",
            "Calcule de la similarité entre les embeddings de dataset avec celui de l'utilisateur\n",
            "Normalisation les valeurs de similartés entre [0,1] et les faire le trié selon l'odre decroissant\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:19:37] \"\u001b[37mPOST /submit_test HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------- New Execution -----------------------------------------------------\n",
            "Loading dataset without shuffling...\n",
            "chargement d'un nouveau modèle de deep learning pré-entrainé par nous...\n",
            "le nom code du modèle de deep learning est: BERT_A\n",
            "Le modèle deep learning n'a pas été changé, en garde le modèle ancien...\n",
            "Les embeddings du dataset pour le modèle BERT existent deja, pas la peine de les créer à nouveau\n",
            "Codification du l'entré de l'utilisateur en embeddings\n",
            "Calcule de la similarité entre les embeddings de dataset avec celui de l'utilisateur\n",
            "Normalisation les valeurs de similartés entre [0,1] et les faire le trié selon l'odre decroissant\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:19:45] \"\u001b[37mGET /index5.html HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 07:20:09] \"\u001b[37mGET /index6.html HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# Flask App\n",
        "from flask import Flask, render_template, request, redirect\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import current_app\n",
        "import pytest\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@pytest.fixture\n",
        "def client():\n",
        "    with app.test_client() as client:\n",
        "        with app.app_context():\n",
        "            assert current_app.config[\"ENV\"] == \"production\"\n",
        "        yield client\n",
        "\n",
        "def test_index_page(client):\n",
        "   response = client.get('/')\n",
        "   assert response.status_code == 200\n",
        "   assert b'Welcome!' in response.data\n",
        "\n",
        "# 1- Rendering Routes --------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "@app.route(\"/\")\n",
        "@app.route(\"/page_principale\")\n",
        "def page():\n",
        "   return render_template(\"page_principale.html\")\n",
        "\n",
        "\n",
        "@app.route(\"/ts\")\n",
        "@app.route(\"/index6.html\")\n",
        "def test():\n",
        "  global data, list_events_test, custom_nn_model\n",
        "  display_first = True\n",
        "  custom_nn_model = False\n",
        "  gc.collect()\n",
        "  #Show test querries, first panel and list enventes\n",
        "  return render_template(\"index6.html\", test_en = data.test_en, test_ar = data.test_ar, display_first = display_first, custom_nn_model=custom_nn_model, list_events_test=list_events_test)\n",
        "\n",
        "\n",
        "\n",
        "@app.route(\"/tr\")\n",
        "@app.route(\"/index5.html\")\n",
        "def train():\n",
        "    return render_template(\"index5.html\")\n",
        "\n",
        "\n",
        "# 2- Data-submitted routes ----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "@app.route(\"/submit_test\", methods=['POST','GET'])\n",
        "def submit_test():\n",
        "  global var_radio_dl_model, var_radio_dl_model_old, data, custom_nn_model, user_dataset, list_events_test, custom_modeltype\n",
        "  display_first = False # Display result similarity\n",
        "  gc.collect()\n",
        "  consolenext()\n",
        "\n",
        "  #- Parse form inputs\n",
        "  if request.form.get('ch_lang')=='on':\n",
        "      langue='en'\n",
        "      phrase_choisi=request.form.get('test_choix_en')\n",
        "  else:\n",
        "      langue='ar' \n",
        "      phrase_choisi=request.form.get('test_choix_ar')\n",
        "  nbr_top_phrase = int(request.form.get('top_n_result'))\n",
        "  var_radio_dl_model=request.form.get('radgroup_pre_model')\n",
        "\n",
        "  # Load concerned dataset (can be user defined or our own)\n",
        "  if custom_nn_model:\n",
        "    data = user_dataset\n",
        "  else:\n",
        "    data = DatasetLoader()\n",
        "    data.load(80/100, False)\n",
        "\n",
        "  # Load selected Deep learning model (LSTM, GRU, BERT)\n",
        "  if not custom_nn_model:\n",
        "    load_deep_learning_model_pre(str(var_radio_dl_model))\n",
        "  \n",
        "  if var_radio_dl_model.startswith('BERT') or custom_modeltype=='bert':\n",
        "     #Evaluation (BERT case)\n",
        "     eval_result = evaluate_model_bert(phrase_choisi, nbr_top_phrase, custom_nn_model)\n",
        "  else:\n",
        "    #Evaluation (LSTM/GRU case)\n",
        "    eval_result = evaluate_model_keras(phrase_choisi, nbr_top_phrase, langue, custom_nn_model)\n",
        "\n",
        "  # Print test querries, evalResults(Similarities) and user selected phrase\n",
        "  return render_template(\"index6.html\",res=eval_result , test_en=data.test_en, test_ar=data.test_ar,phrase_choisi=phrase_choisi, display_first=display_first, custom_nn_model=custom_nn_model, list_events_test=list_events_test)\n",
        "\n",
        "\n",
        "@app.route('/createNN', methods=['GET', 'POST'])\n",
        "def createNN():\n",
        "  global user_dataset, custom_modeltype,custom_nn_bert\n",
        "  #-ALL: Get User's inputs\n",
        "  nn_model_type, w2v_model_type , nbr_layers, nbr_in_layer1,\\\n",
        "  nbr_in_layer2, nbr_in_layer3, nbr_in_layer4, nbr_in_layer5,\\\n",
        "  dense, weights, dropout, activ_fun, learn_rate, momentum, decay_rate,\\\n",
        "  loss_fun, epochs, batchsize, optim, split_ratio, embeddings_length = parse_request(request)\n",
        "  \n",
        "  #-ALL: Load new instance of dataset STS for user with randomizing\n",
        "  consolesep()\n",
        "  consolesep()\n",
        "  consoleln('Begin training user\\'s custom model', 'lime')\n",
        "  consoleln('Loading a new instance of STS dataset for User ...')\n",
        "  user_dataset = DatasetLoader()\n",
        "  user_dataset.load(float(split_ratio)/100, random = True)\n",
        "  consoleln('STS dataset loaded!')\n",
        "  \n",
        "  console('There are')\n",
        "  console(str(user_dataset.size) +'instances', 'pink')\n",
        "  console('The split data goes:')\n",
        "  console(str(user_dataset.train_size)+' for training', 'pink')\n",
        "  console('and')\n",
        "  consoleln(str(user_dataset.test_size)+'for testing', 'pink')\n",
        "\n",
        "  #-LSTM|GRU: shares the sames structure (from KERAS), BERT is a transformer model\n",
        "  if nn_model_type == 'LSTM' or nn_model_type == 'GRU':\n",
        "\n",
        "    #-LSTM|GRU: Load User's W2v choice\n",
        "    consoleln('Loading user\\'s Word Embedings ...', 'yellow')\n",
        "    load_w2v_model(w2v_model_type)\n",
        "\n",
        "    # -LSTM|GRU:Preprocess dataset to create embeddings\n",
        "    consoleln('Trying to embed trainset...', 'yellow')\n",
        "    user_train_embd_en, user_train_embd_ar = preprocess(user_dataset.train_en, user_dataset.train_ar)\n",
        "    consoleln('done!', 'green')\n",
        "    consoleln('Trying to embed testset...', 'yellow')\n",
        "    user_test_embd_en, user_test_embd_ar = preprocess(user_dataset.test_en, user_dataset.test_ar)\n",
        "    consoleln('done!', 'green')\n",
        "\n",
        "    #-LSTM|GRU: Building a custom neural network model\n",
        "    consoleln('assembling User\\'s custon neural network into shape...', 'yellow')\n",
        "\n",
        "    #-LSTM|GRU: 1- init Loss function\n",
        "    lstm_gru_loss_fun = get_keras_loss_fun(loss_fun)\n",
        "        \n",
        "    #-LSTM|GRU: 2- init Learning rate + it's decay rate\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=float(learn_rate),\n",
        "        decay_steps=100000,\n",
        "        decay_rate=float(decay_rate))\n",
        "\n",
        "    #-LSTM|GRU: 3- init optimizer, note that only SGD and RMSPROP use momentum\n",
        "    lstm_gru_optim = get_keras_optimizer(optim, lr_schedule, momentum)\n",
        "    \n",
        "    #-LSTM|GRU: 4- init weights\n",
        "    weights_init = tf.keras.initializers.RandomUniform(minval=-float(weights), maxval=float(weights), seed=None)\n",
        "\n",
        "    #-LSTM|GRU: Deleting models (if they exists)\n",
        "    try: \n",
        "      del custom_nn\n",
        "    except NameError:\n",
        "      pass\n",
        "    try: \n",
        "      del custom_nn_reverse\n",
        "    except NameError:\n",
        "      pass\n",
        "\n",
        "    #-LSTM|GRU: Adding layers to the custom model\n",
        "    custom_nn = Sequential()\n",
        "    if nn_model_type == 'LSTM':\n",
        "      custom_nn.add(LSTM(int(nbr_in_layer1), input_shape=(maxlength,300), return_sequences=True, activation = activ_fun, kernel_initializer = weights_init))\n",
        "      custom_nn.add(Dropout(float(dropout)))\n",
        "      if int(nbr_layers)>1:\n",
        "        custom_nn.add(Dense(int(dense)))\n",
        "        custom_nn.add(LSTM(int(nbr_in_layer2), input_shape=(maxlength,300), return_sequences=True, activation = activ_fun, kernel_initializer = weights_init))\n",
        "        custom_nn.add(Dropout(float(dropout)))\n",
        "      if int(nbr_layers)>2:\n",
        "        custom_nn.add(Dense(int(dense)))\n",
        "        custom_nn.add(LSTM(int(nbr_in_layer3), input_shape=(maxlength,300), return_sequences=True, activation = activ_fun, kernel_initializer = weights_init))\n",
        "        custom_nn.add(Dropout(float(dropout)))\n",
        "      if int(nbr_layers)>3:\n",
        "        custom_nn.add(Dense(int(dense)))\n",
        "        custom_nn.add(LSTM(int(nbr_in_layer4), input_shape=(maxlength,300), return_sequences=True, activation = activ_fun, kernel_initializer = weights_init))\n",
        "        custom_nn.add(Dropout(float(dropout)))\n",
        "      if int(nbr_layers)>4:\n",
        "        custom_nn.add(Dense(int(dense)))\n",
        "        custom_nn.add(LSTM(int(nbr_in_layer5), input_shape=(maxlength,300), return_sequences=True, activation = activ_fun, kernel_initializer = weights_init))\n",
        "        custom_nn.add(Dropout(float(dropout)))\n",
        "      custom_nn.add(Dense(300, activation = activ_fun)) #Final dense layer\n",
        "    elif nn_model_type == 'GRU':\n",
        "      custom_nn.add(GRU(int(nbr_in_layer1), input_shape=(maxlength,300), return_sequences=True, activation = activ_fun, kernel_initializer = weights_init))\n",
        "      custom_nn.add(Dropout(float(dropout)))\n",
        "      if int(nbr_layers)>1:\n",
        "        custom_nn.add(Dense(int(dense)))\n",
        "        custom_nn.add(GRU(int(nbr_in_layer2), input_shape=(maxlength,300), return_sequences=True, activation = activ_fun, kernel_initializer = weights_init))\n",
        "        custom_nn.add(Dropout(float(dropout)))\n",
        "      if int(nbr_layers)>2:\n",
        "        custom_nn.add(Dense(int(dense)))\n",
        "        custom_nn.add(GRU(int(nbr_in_layer3), input_shape=(maxlength,300), return_sequences=True, activation = activ_fun, kernel_initializer = weights_init))\n",
        "        custom_nn.add(Dropout(float(dropout)))\n",
        "      if int(nbr_layers)>3:\n",
        "        custom_nn.add(Dense(int(dense)))\n",
        "        custom_nn.add(GRU(int(nbr_in_layer4), input_shape=(maxlength,300), return_sequences=True, activation = activ_fun, kernel_initializer = weights_init))\n",
        "        custom_nn.add(Dropout(float(dropout)))\n",
        "      if int(nbr_layers)>4:\n",
        "        custom_nn.add(Dense(int(dense)))\n",
        "        custom_nn.add(GRU(int(nbr_in_layer5), input_shape=(maxlength,300), return_sequences=True, activation = activ_fun, kernel_initializer = weights_init))\n",
        "        custom_nn.add(Dropout(float(dropout)))\n",
        "      custom_nn.add(Dense(300, activation = activ_fun)) #Same final dense layer\n",
        "\n",
        "    #-LSTM|GRU: Start training the custom neural network model\n",
        "    custom_nn_reverse = tf.keras.models.clone_model(custom_nn)\n",
        "\n",
        "    consoleln('Training the first'+nn_model_type+'model to learn and predict en- > ar', 'magenta')\n",
        "    custom_nn.compile(optimizer=lstm_gru_optim, loss=lstm_gru_loss_fun ,metrics=['accuracy'])\n",
        "    custom_nn.summary()\n",
        "    custom_nn_history=custom_nn.fit(user_train_embd_en, user_train_embd_ar, validation_data=(user_test_embd_en, user_test_embd_ar), epochs=int(epochs), batch_size=int(batchsize)*len(user_train_embd_ar))\n",
        "    \n",
        "    consoleln('Training the second'+nn_model_type+'model to learn and predict ar- > en', 'magenta')\n",
        "    custom_nn_reverse.compile(optimizer=lstm_gru_optim, loss=lstm_gru_loss_fun ,metrics=['accuracy'])\n",
        "    custom_nn_reverse.summary()\n",
        "    custom_nn_reverse_history=custom_nn.fit(user_train_embd_ar, user_train_embd_en, validation_data=(user_test_embd_ar, user_test_embd_en), epochs=int(epochs), batch_size=int(batchsize)*len(user_train_embd_ar))\n",
        "    custom_modeltype='lstm_gru'\n",
        "  else: # We creating a BERT Model\n",
        "    consoleln('Downloading a new BERT model for user to train', 'yellow')\n",
        "    from sentence_transformers import SentenceTransformer, models\n",
        "    custom_nn_bert = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
        "\n",
        "    #-BERT: prepare training and test samples, structure is like this: \n",
        "    #[['arabic text1', 'corresponding english text1' 'score lbl'], [next], [ect], ...]\n",
        "    train_samples, test_samples = [], []\n",
        "      \n",
        "    for i in range(len(data.train_en)):\n",
        "      train_samples.append(InputExample(texts=[data.train_en[i], data.train_ar[i]], label = data.train_score[i]/5)) # normalise [0..1]\n",
        "    train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=int(30)*data.train_size//100)\n",
        "\n",
        "    for i in range(len(data.test_en)):\n",
        "      test_samples.append(InputExample(texts=[data.test_en[i], data.test_ar[i]], label = data.test_score[i]/5))\n",
        "    #dev_evaluator_sts = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
        "\n",
        "    #-BERT: Redy loss function\n",
        "    bert_loss_function = get_bert_loss_fun(custom_nn_bert, 'CosineSimilarityLoss') \n",
        "    #-BERT: init activation function\n",
        "    bert_activ_fun = get_bert_active_fun(activ_fun)\n",
        "    #-BERT: Redy loss function\n",
        "    bert_loss_function = get_bert_loss_fun(custom_nn_bert, loss_fun)\n",
        "    \n",
        "    #-BERT: Fine tuning the model\n",
        "    custom_nn_bert.fit(\n",
        "        train_objectives=[(train_dataloader, bert_loss_function)], \n",
        "        epochs=int(epochs), \n",
        "        show_progress_bar =True, \n",
        "        warmup_steps=1000,\n",
        "        evaluation_steps=1000\n",
        "    )\n",
        "\n",
        "    #-BERT: Freeing GPU memory\n",
        "    cuda.empty_cache()\n",
        "    custom_modeltype = 'bert'\n",
        "\n",
        "  display_first= True\n",
        "  global custom_nn_model\n",
        "  custom_nn_model = True\n",
        "  return  render_template('/index6.html', test_ar=user_dataset.test_ar, test_en = user_dataset.test_en,  display_first = display_first, custom_nn_model=custom_nn_model, list_events_test=list_events_test)\n",
        "  \n",
        "# use this route to shutdown the server from ngrok side, without leaving open endpoints\n",
        "@app.route('/sh')\n",
        "def sh():\n",
        "    shutdown_server = request.environ.get('werkzeug.server.shutdown')\n",
        "    if shutdown_server is None:\n",
        "        raise RuntimeError('Not running with the Werkzeug Server')\n",
        "    shutdown_server()\n",
        "    return 'Server shutting down...'\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  os.chdir('/content')\n",
        "  allow_colab_print = True\n",
        "  if first_run:\n",
        "    first_run = False\n",
        "  else:\n",
        "    list_events_test=list()\n",
        "    gc.collect()\n",
        "  app.run()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lm3RuanwJI5"
      },
      "source": [
        "#Failsafe (Do not use)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5C-LZeELcvW"
      },
      "outputs": [],
      "source": [
        "%reset # del all vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywTGrAt7hvqk"
      },
      "outputs": [],
      "source": [
        "# In case you see a log like this, run this celle to reset the 'request' variable and tp delete the current app.\n",
        "\"\"\"\n",
        "ERROR:root:Unexpected exception finding object shape\n",
        "Traceback (most recent call last):\n",
        "  File \"/usr/local/lib/python3.7/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
        "    shape = getattr(obj, 'shape', None)\n",
        "  File \"/usr/local/lib/python3.7/dist-packages/werkzeug/local.py\", line 347, in __getattr__\n",
        "    return getattr(self._get_current_object(), name)\n",
        "  File \"/usr/local/lib/python3.7/dist-packages/werkzeug/local.py\", line 306, in _get_current_object\n",
        "    return self.__local()\n",
        "  File \"/usr/local/lib/python3.7/dist-packages/flask/globals.py\", line 38, in _lookup_req_object\n",
        "    raise RuntimeError(_request_ctx_err_msg)\n",
        "RuntimeError: Working outside of request context.\n",
        "\n",
        "This typically means that you attempted to use functionality that needed\n",
        "an active HTTP request.  Consult the documentation on testing for\n",
        "information about how to avoid this problem.\n",
        "\"\"\"\n",
        "try:\n",
        "  del request\n",
        "except NameError:\n",
        "  pass\n",
        "try:\n",
        "  del app\n",
        "except NameError:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqGYEffA3xWm"
      },
      "outputs": [],
      "source": [
        "# Release GPU Memory\n",
        "import gc\n",
        "gc.collect()\n",
        "cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxTqboVe8PlK"
      },
      "source": [
        "# BERT, going anew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Dl7nFu28Rlg"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#-BERT: Download bert's embeddings\n",
        "bert_word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=int(embeddings_length))\n",
        "\n",
        "#-BERT: init internal module for bert model\n",
        "pooling_model = models.Pooling(bert_word_embedding_model.get_word_embedding_dimension())\n",
        "\n",
        "#-BERT: init activation function\n",
        "bert_activ_fun = get_bert_active_fun(activ_fun)\n",
        "\n",
        "#-BERT: link everything into dense\n",
        "dense_model = models.Dense(\n",
        "    in_features=pooling_model.get_sentence_embedding_dimension(),\n",
        "    out_features=int(embeddings_length), \n",
        "    activation_function=bert_activ_fun\n",
        "    )\n",
        "\n",
        "#-BERT: Create a new BERT model containing all modules previously created\n",
        "bert_model = SentenceTransformer(\n",
        "    modules=[bert_word_embedding_model, pooling_model, dense_model], \n",
        "    device='cuda:0'\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, models\n",
        "modelB = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
        "\n",
        "\n",
        "#-BERT: prepare training and test samples, structure is like this [['arabic text1', 'corresponding english text1' 'similarity'], [next], [ect], ...]\n",
        "train_samples, test_samples = [], []\n",
        "for i in range(len(data.train_en)):\n",
        "  train_samples.append(InputExample(texts=[data.train_en[i], data.train_ar[i]], label = data.train_score[i]/5)) # normalise [0..1]\n",
        "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=int(30)*data.train_size//100)\n",
        "\n",
        "for i in range(len(data.test_en)):\n",
        "  test_samples.append(InputExample(texts=[data.test_en[i], data.test_ar[i]], label = data.test_score[i]/5))\n",
        "#dev_evaluator_sts = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
        "\n",
        "#-BERT: Redy loss function\n",
        "bert_loss_function = get_bert_loss_fun(modelB, 'CosineSimilarityLoss') \n",
        "\n",
        "#-BERT: Fine tuning the model\n",
        "modelB.fit(\n",
        "    train_objectives=[(train_dataloader, bert_loss_function)], \n",
        "    epochs=int(100), \n",
        "    show_progress_bar =True, \n",
        "    warmup_steps=1000,\n",
        "    evaluation_steps=1000\n",
        ")\n",
        "\n",
        "#-BERT: Freeing GPU memory\n",
        "cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVgfrFKeh7UY"
      },
      "source": [
        "##save model bert\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-a6KUc4h6f9"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/Web/pre_entrainer')\n",
        "modelB.save(\"modelbert_100ep_batch30.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BZVRBuqZZcG"
      },
      "outputs": [],
      "source": [
        "modelB = SentenceTransformer('/content/drive/MyDrive/Colab Notebooks/Web/pre_entrainer/modelbert_100ep_batch30.h5/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_FbR1HCD46P"
      },
      "source": [
        "## Calculate similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "nNgGxCJ8D6dd",
        "outputId": "36d54851-698d-4eb4-8834-f632b57fcd2b"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-1396f5cc9097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Compute embedding for both lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0membeddings1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_nn_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines_ar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0membeddings2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_nn_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Batches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0msentences_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mTokenizes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \"\"\"\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_sentence_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'tokenize'"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Two lists of sentences\n",
        "sentences1 = ['two men are joying']\n",
        "\n",
        "sentences2 = ['رجلان يضحكان']\n",
        "\n",
        "#Compute embedding for both lists\n",
        "embeddings1 = custom_nn_bert.encode(data.lines_ar, convert_to_tensor=True)\n",
        "embeddings2 = custom_nn_bert.encode(data.lines_en, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities\n",
        "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "#Output the pairs with their score\n",
        "for i in range(len(data.lines_ar)):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(data.lines_ar[i], data.lines_en[i], cosine_scores[i][i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_h-1AOSE6qT"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "use_input = ['رجلان يضحكان ']\n",
        "\n",
        "#Compute embedding for both lists\n",
        "bert_embeddings_ar = modelB.encode(data.lines_ar, convert_to_tensor=True)\n",
        "bert_embeddings_en = modelB.encode(data.lines_en, convert_to_tensor=True)\n",
        "bert_user_embeddings = modelB.encode(use_input, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities\n",
        "cosine_scores_ar = util.cos_sim(bert_embeddings_ar, bert_user_embeddings)\n",
        "cosine_scores_en = util.cos_sim(bert_embeddings_en, bert_user_embeddings)\n",
        "cosine_scores = torch.cat((cosine_scores_ar, cosine_scores_en))\n",
        "cosine_scores, _ = torch.sort(cosine_scores)\n",
        "#T = torch.cat((T1,T2,T3))\n",
        "#v = torch.sort(T)\n",
        "\n",
        "printdata = data.lines_ar+data.lines_en\n",
        "print( len(cosine_scores))\n",
        "#Output the pairs with their score\n",
        "for i in range(0, len(cosine_scores), 2):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(printdata[i], use_input[0], cosine_scores[i][0]))\n",
        "\n",
        "\"\"\"\n",
        "#Output the pairs with their score\n",
        "for i in range(len(data.lines_en)):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(data.lines_en[i], use_input[0], cosine_scores_en[i][0]))\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1jYO5vteHmX"
      },
      "outputs": [],
      "source": [
        "\n",
        "cosine_scores_ar = util.cos_sim(bert_embeddings_ar, bert_user_embeddings)\n",
        "cosine_scores_en = util.cos_sim(bert_embeddings_en, bert_user_embeddings)\n",
        "\n",
        "#Concat two tensors results ar+en\n",
        "concated = torch.cat((cosine_scores_ar, cosine_scores_en))\n",
        "\n",
        "#Sort while keeping in mind swaping the order of dataset lines too\n",
        "cosine_scores = sorted(zip(concated, data.lines_ar+data.lines_en), reverse = True)\n",
        "\n",
        "# normalizing similarity into [0, 1] instead of [-1, 1] & rewriting evrything into a good looking list\n",
        "cosine_scores = [(y, float(\"{:.4f}\".format((x[0].item()+1)/2))) for x,y in cosine_scores]\n",
        "\n",
        "#Assertion check\n",
        "for x in cosine_scores:\n",
        "  assert float(x[1])>=0 and float(x[1]) <=1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4T2GWE3H5B8"
      },
      "source": [
        "## Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGbiBFmJNkWN"
      },
      "outputs": [],
      "source": [
        "evaluators = []\n",
        "\n",
        "\"\"\"\n",
        "test_evaluator = evaluation.EmbeddingSimilarityEvaluator(bert_embeddings_en, bert_embeddings_ar, data.lines_score, batch_size = 20, name='AR-EN', show_progress_bar=True)\n",
        "evaluators.append(test_evaluator)\n",
        "\"\"\"\n",
        "test_evaluator = evaluation.EmbeddingSimilarityEvaluator(data.lines_ar, data.lines_en, data.lines_score, batch_size = 30, name='resultsss', show_progress_bar=False, write_csv = True)\n",
        "\n",
        "\n",
        "modelB.fit(\n",
        "    evaluator=test_evaluator,\n",
        "    train_objectives=[(train_dataloader, bert_loss_function)], \n",
        "    epochs=int(100), \n",
        "    show_progress_bar =True, \n",
        "    warmup_steps=1000,\n",
        "    evaluation_steps=1000\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae1JMg4STNh_"
      },
      "outputs": [],
      "source": [
        "\n",
        "os.chdir(\"/content\")\n",
        "test_evaluator = evaluation.EmbeddingSimilarityEvaluator(data.lines_ar, data.lines_en, data.lines_score, batch_size = 30, name='AR-EN', show_progress_bar=True, write_csv = True)\n",
        "#def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1) \n",
        "for i in range(100):\n",
        "  test_evaluator(modelB,'',i, 100 )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qsMsA5p2dM9X",
        "k5CoynMDdU6h",
        "X0RjwQVPNNQw",
        "A5LHhPCYm2O_",
        "LwSAAuk8da7k",
        "RmZu3TlTgwVu",
        "brr63eZ0CsIB",
        "yAQeE42rdlUQ",
        "xFDuWd-6doFH",
        "VglWAMwWfMFq",
        "MfM5QmtEfQDB",
        "7Lm3RuanwJI5"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}